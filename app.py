"""
åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼‰
éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ - å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æŠ½å‡º
"""

import base64
import concurrent.futures
import io
import json
import os
import re
import threading
import time
from collections import OrderedDict
from pathlib import Path
from typing import Optional

import pandas as pd
import streamlit as st
from dotenv import load_dotenv

# app.pyã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®.envã‚’æ˜ç¤ºçš„ã«èª­ã¿è¾¼ã‚€ï¼ˆã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒå¤‰æ•°ã‚ˆã‚Šå„ªå…ˆï¼‰
load_dotenv(Path(__file__).parent / ".env", override=True)

# =============================================================================
# å®šæ•°
# =============================================================================

CSV_COLUMNS = [
    "äº‹æ¥­",
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "åˆ©ç”¨è€…_ã›ã„",
    "åˆ©ç”¨è€…_ã‚ã„",
    "æ€§åˆ¥",
    "ç”Ÿå¹´æœˆæ—¥",
    "ä¿è­·è€…_å§“",
    "ä¿è­·è€…_å",
    "å¥‘ç´„æ—¥",
    "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—",
    "å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥",
    "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨",
    "éƒµä¾¿ç•ªå·",
    "éƒ½é“åºœçœŒ",
    "ä½æ‰€",
    "é›»è©±ç•ªå·",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹",
]

REQUIRED_FIELDS = [
    "äº‹æ¥­",
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "åˆ©ç”¨è€…_ã›ã„",
    "åˆ©ç”¨è€…_ã‚ã„",
    "æ€§åˆ¥",
    "å¥‘ç´„æ—¥",
    "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—",
    "å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥",
    "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨",
]

EXTRACTION_PROMPT = """ã‚ãªãŸã¯éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹ã®æ›¸é¡èª­ã¿å–ã‚Šå°‚é–€ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

## æ›¸é¡ç¨®åˆ¥ã®åˆ¤å®š
ã¾ãšã€ç”»åƒãŒä»¥ä¸‹ã®ã©ã¡ã‚‰ã®æ›¸é¡ã‹åˆ¤å®šã—ã¦ãã ã•ã„:
- **å—çµ¦è€…è¨¼**: å—çµ¦è€…è¨¼ç•ªå·ã€æ”¯çµ¦æ±ºå®šæœŸé–“ã€ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æœŸé–“ã€åˆ©ç”¨è€…ã®ä½æ‰€ç­‰ãŒè¨˜è¼‰ã•ã‚ŒãŸå…¬çš„æ›¸é¡
- **åˆ©ç”¨å¥‘ç´„æ›¸**: ã€Œåˆ©ç”¨å¥‘ç´„æ›¸ã€ã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ã€å¥‘ç´„æ¡é …ã€æœ«å°¾ã«ç½²åæ¬„ã¨ã€Œä»¤å’Œâ—‹å¹´â—‹æœˆâ—‹æ—¥ã€ã®æ—¥ä»˜ãŒã‚ã‚‹æ›¸é¡

åˆ¤å®šçµæœã‚’ "æ›¸é¡ç¨®åˆ¥" ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼ˆ"å—çµ¦è€…è¨¼" or "åˆ©ç”¨å¥‘ç´„æ›¸"ï¼‰ã€‚

## æŠ½å‡ºãƒ«ãƒ¼ãƒ«
- è©²å½“ã™ã‚‹æ›¸é¡ã«è¨˜è¼‰ã®ã‚ã‚‹é …ç›®ã‚’èª­ã¿å–ã£ã¦ãã ã•ã„
- ãã®æ›¸é¡ã«è©²å½“ã—ãªã„é …ç›®ã¯ç©ºæ–‡å­—("")ã¨ã—ã¦ãã ã•ã„
- èª­ã¿å–ã‚Œãªã„é …ç›®ã‚‚ç©ºæ–‡å­—("")ã¨ã—ã¦ãã ã•ã„
- å„é …ç›®ã®ç¢ºä¿¡åº¦ã‚’ "confidence" ã«è¨˜è¼‰: "high"=ã¯ã£ãã‚Šèª­ã‚ãŸ, "low"=ä¸é®®æ˜ãƒ»æ¨æ¸¬

### å—çµ¦è€…è¨¼ã‹ã‚‰èª­ã¿å–ã‚‹é …ç›®:
äº‹æ¥­, åˆ©ç”¨è€…_å§“, åˆ©ç”¨è€…_å, åˆ©ç”¨è€…_ã›ã„, åˆ©ç”¨è€…_ã‚ã„, æ€§åˆ¥, ç”Ÿå¹´æœˆæ—¥,
ä¿è­·è€…_å§“, ä¿è­·è€…_å, å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—, å—çµ¦è€…è¨¼ç•ªå·, æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥,
æ”¯çµ¦æ±ºå®šæº€äº†æ—¥, ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ, ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨,
éƒµä¾¿ç•ªå·, éƒ½é“åºœçœŒ, ä½æ‰€, é›»è©±ç•ªå·, ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹

### åˆ©ç”¨å¥‘ç´„æ›¸ã‹ã‚‰èª­ã¿å–ã‚‹é …ç›®:
åˆ©ç”¨è€…_å§“, åˆ©ç”¨è€…_å, å¥‘ç´„æ—¥ï¼ˆç½²åæ¬„ã®ã€Œä»¤å’Œâ—‹å¹´â—‹æœˆâ—‹æ—¥ã€ã®æ—¥ä»˜ï¼‰

## é …ç›®ã®è£œè¶³
- äº‹æ¥­: ã€Œè¨ˆç”»ç›¸è«‡æ”¯æ´ã€ã€Œéšœå®³å…ç›¸è«‡æ”¯æ´ã€ãªã©
- åˆ©ç”¨è€…_ã›ã„/ã‚ã„: å§“åã®ã²ã‚‰ãŒãªèª­ã¿
- ç”Ÿå¹´æœˆæ—¥: ä¾‹ 1990å¹´01æœˆ15æ—¥
- ä¿è­·è€…_å§“/å: å…ç«¥ã®å ´åˆã®ã¿ï¼ˆè©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—ï¼‰
- å¥‘ç´„æ—¥: åˆ©ç”¨å¥‘ç´„æ›¸ã®ç½²åæ¬„ã®æ—¥ä»˜ã€‚å’Œæš¦ã§ãã®ã¾ã¾è¨˜è¼‰ï¼ˆä¾‹: ä»¤å’Œ6å¹´4æœˆ1æ—¥ï¼‰
- å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—: ã€ŒéšœãŒã„ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ã€ã€Œåœ°åŸŸç›¸è«‡æ”¯æ´å—çµ¦è€…è¨¼ã€ã€ŒéšœãŒã„å…é€šæ‰€å—çµ¦è€…è¨¼ã€ã®ã„ãšã‚Œã‹
- å—çµ¦è€…è¨¼ç•ªå·: 10æ¡ã®æ•°å­—
- æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥/æº€äº†æ—¥: ä¾‹ 2023å¹´02æœˆ05æ—¥
- ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ / æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨: æ•°å­—ã®ã¿ï¼ˆä¾‹: 3, 6ï¼‰
- éƒµä¾¿ç•ªå·: ãƒã‚¤ãƒ•ãƒ³ãªã—7æ¡ï¼ˆä¾‹: 8120011ï¼‰
- é›»è©±ç•ªå·: ãƒã‚¤ãƒ•ãƒ³ä»˜ãOKï¼ˆä¾‹: 092-710-4570ï¼‰

## å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ã€èª¬æ˜ä¸è¦ï¼‰
{
  "æ›¸é¡ç¨®åˆ¥": "",
  "äº‹æ¥­": "",
  "åˆ©ç”¨è€…_å§“": "",
  "åˆ©ç”¨è€…_å": "",
  "åˆ©ç”¨è€…_ã›ã„": "",
  "åˆ©ç”¨è€…_ã‚ã„": "",
  "æ€§åˆ¥": "",
  "ç”Ÿå¹´æœˆæ—¥": "",
  "ä¿è­·è€…_å§“": "",
  "ä¿è­·è€…_å": "",
  "å¥‘ç´„æ—¥": "",
  "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—": "",
  "å—çµ¦è€…è¨¼ç•ªå·": "",
  "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥": "",
  "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨": "",
  "éƒµä¾¿ç•ªå·": "",
  "éƒ½é“åºœçœŒ": "",
  "ä½æ‰€": "",
  "é›»è©±ç•ªå·": "",
  "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹": "",
  "confidence": {
    "äº‹æ¥­": "high",
    "åˆ©ç”¨è€…_å§“": "high",
    "åˆ©ç”¨è€…_å": "high",
    "åˆ©ç”¨è€…_ã›ã„": "low",
    "åˆ©ç”¨è€…_ã‚ã„": "low",
    "æ€§åˆ¥": "high",
    "ç”Ÿå¹´æœˆæ—¥": "high",
    "ä¿è­·è€…_å§“": "low",
    "ä¿è­·è€…_å": "low",
    "å¥‘ç´„æ—¥": "high",
    "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—": "high",
    "å—çµ¦è€…è¨¼ç•ªå·": "high",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥": "high",
    "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥": "high",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ": "low",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨": "low",
    "éƒµä¾¿ç•ªå·": "high",
    "éƒ½é“åºœçœŒ": "high",
    "ä½æ‰€": "high",
    "é›»è©±ç•ªå·": "low",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹": "low"
  }
}
"""

MAX_FILES = 10
MAX_IMAGE_BYTES = 4_500_000  # base64å¤‰æ›å¾Œã«5MBä»¥å†…ã«åã¾ã‚‹ã‚ˆã†ä½™è£•ã‚’æŒãŸã›ã‚‹

# =============================================================================
# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆå–å¾—
# =============================================================================


def get_secret(key: str) -> str:
    """Streamlit Cloud ã® secrets â†’ .env ã®é †ã§ã‚­ãƒ¼ã‚’å–å¾—"""
    try:
        return st.secrets[key]
    except (KeyError, FileNotFoundError):
        return os.getenv(key, "")


# =============================================================================
# ç”»åƒå‡¦ç†
# =============================================================================


def compress_image(image_bytes: bytes, media_type: str) -> tuple[bytes, str]:
    """ç”»åƒãŒAPIã®ä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆã«ãƒªã‚µã‚¤ã‚ºãƒ»åœ§ç¸®ã™ã‚‹"""
    from PIL import Image

    if len(image_bytes) <= MAX_IMAGE_BYTES:
        return image_bytes, media_type

    img = Image.open(io.BytesIO(image_bytes))

    for quality in (85, 70, 50, 35):
        max_dim = 2048 if quality >= 70 else 1600
        if max(img.size) > max_dim:
            img.thumbnail((max_dim, max_dim), Image.LANCZOS)

        buf = io.BytesIO()
        rgb_img = img.convert("RGB") if img.mode != "RGB" else img
        rgb_img.save(buf, format="JPEG", quality=quality)
        result = buf.getvalue()
        if len(result) <= MAX_IMAGE_BYTES:
            return result, "image/jpeg"

    img.thumbnail((1200, 1200), Image.LANCZOS)
    buf = io.BytesIO()
    img.convert("RGB").save(buf, format="JPEG", quality=30)
    return buf.getvalue(), "image/jpeg"


def convert_pdf_to_image(pdf_bytes: bytes) -> Optional[bytes]:
    """PDFã®1ãƒšãƒ¼ã‚¸ç›®ã‚’ç”»åƒã«å¤‰æ›"""
    try:
        from pdf2image import convert_from_bytes

        images = convert_from_bytes(pdf_bytes, first_page=1, last_page=1, dpi=200)
        if images:
            buf = io.BytesIO()
            images[0].save(buf, format="PNG")
            return buf.getvalue()
    except ImportError:
        st.error(
            "pdf2imageãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            "ã¾ãŸã€Popplerã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚‚å¿…è¦ã§ã™ã€‚\n"
            "Windows: https://github.com/oschwartz10612/poppler-windows/releases\n"
            "Mac: `brew install poppler`\n"
            "Linux: `sudo apt-get install poppler-utils`"
        )
    except Exception as e:
        st.error(f"PDFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    return None


def get_media_type(filename: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ MIME ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
    ext = filename.lower().rsplit(".", 1)[-1]
    return "image/jpeg" if ext in ("jpg", "jpeg") else "image/png"


# =============================================================================
# AI æŠ½å‡º
# =============================================================================


def parse_json_response(text: str) -> Optional[dict]:
    """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    if "```" in text:
        start = text.find("```")
        end = text.rfind("```")
        if start != end:
            json_block = text[start:end]
            first_newline = json_block.find("\n")
            if first_newline != -1:
                json_block = json_block[first_newline + 1:]
            try:
                return json.loads(json_block.strip())
            except json.JSONDecodeError:
                pass

    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end > start:
        try:
            return json.loads(text[start:end + 1])
        except json.JSONDecodeError:
            pass

    st.error("AIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’JSONå½¢å¼ã§è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.code(text, language="text")
    return None


def _convert_wareki_to_seireki(text: str) -> str:
    """å’Œæš¦ï¼ˆä»¤å’Œ/å¹³æˆ/æ˜­å’Œï¼‰ã‚’YYYYå¹´MMæœˆDDæ—¥å½¢å¼ã«å¤‰æ›"""
    era_map = {"ä»¤å’Œ": 2018, "å¹³æˆ": 1988, "æ˜­å’Œ": 1925}
    m = re.match(r"(ä»¤å’Œ|å¹³æˆ|æ˜­å’Œ)\s*(\d+)\s*å¹´\s*(\d+)\s*æœˆ\s*(\d+)\s*æ—¥", text.strip())
    if m:
        era, y, month, day = m.group(1), int(m.group(2)), int(m.group(3)), int(m.group(4))
        year = era_map[era] + y
        return f"{year}å¹´{month:02d}æœˆ{day:02d}æ—¥"
    return text


def _postprocess_extraction(result: dict) -> dict:
    """æŠ½å‡ºçµæœã®å¾Œå‡¦ç†ï¼ˆéƒµä¾¿ç•ªå·ãƒã‚¤ãƒ•ãƒ³é™¤å»ã€å’Œæš¦å¤‰æ›ãªã©ï¼‰"""
    # éƒµä¾¿ç•ªå·ãƒã‚¤ãƒ•ãƒ³é™¤å»
    postal = str(result.get("éƒµä¾¿ç•ªå·", ""))
    if postal:
        result["éƒµä¾¿ç•ªå·"] = re.sub(r"[^\d]", "", postal)

    # æ—¥ä»˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å’Œæš¦â†’è¥¿æš¦å¤‰æ›
    for date_field in ["å¥‘ç´„æ—¥", "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥", "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥", "ç”Ÿå¹´æœˆæ—¥"]:
        val = str(result.get(date_field, "")).strip()
        if val:
            result[date_field] = _convert_wareki_to_seireki(val)

    return result


def extract_with_anthropic(image_base64: str, media_type: str) -> Optional[dict]:
    """Anthropic Claude Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        import anthropic

        api_key = get_secret("ANTHROPIC_API_KEY")
        if not api_key or api_key.startswith("sk-ant-xxx"):
            st.error("ANTHROPIC_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = anthropic.Anthropic(api_key=api_key)
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image", "source": {"type": "base64", "media_type": media_type, "data": image_base64}},
                    {"type": "text", "text": EXTRACTION_PROMPT},
                ],
            }],
        )
        result = parse_json_response(message.content[0].text)
        return _postprocess_extraction(result) if result else None
    except Exception as e:
        st.error(f"Anthropic API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def extract_with_openai(image_base64: str, media_type: str) -> Optional[dict]:
    """OpenAI GPT-4 Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        from openai import OpenAI

        api_key = get_secret("OPENAI_API_KEY")
        if not api_key or api_key.startswith("sk-xxx"):
            st.error("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-4o",
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:{media_type};base64,{image_base64}"}},
                    {"type": "text", "text": EXTRACTION_PROMPT},
                ],
            }],
        )
        result = parse_json_response(response.choices[0].message.content)
        return _postprocess_extraction(result) if result else None
    except Exception as e:
        st.error(f"OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆä¿¡é ¼åº¦ãƒ»çªåˆãƒ»DataFrameæ§‹ç¯‰ï¼‰
# =============================================================================


def calc_confidence(data: dict) -> tuple[int, str, list[str]]:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ç…§åˆç‡(%)ãƒ»åˆ¤å®šãƒ©ãƒ™ãƒ«ãƒ»è¦ç¢ºèªé …ç›®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    confidence_map = data.get("confidence", {})
    total_weight = 0
    earned = 0.0
    low_fields = []

    for col in CSV_COLUMNS:
        weight = 2 if col in REQUIRED_FIELDS else 1
        total_weight += weight
        val = str(data.get(col, "")).strip()
        ai_conf = confidence_map.get(col, "high" if val else "low")

        if not val:
            low_fields.append(col)
        elif ai_conf == "low":
            earned += weight * 0.5
            low_fields.append(col)
        else:
            earned += weight

    pct = int(earned / total_weight * 100) if total_weight else 0

    if pct >= 90 and not any(col in low_fields for col in REQUIRED_FIELDS):
        label = "OK"
    elif pct >= 60:
        label = "è¦ç¢ºèª"
    else:
        label = "è¦ç¢ºèª(ä½)"

    return pct, label, low_fields


def is_record_ok(pct: int, low_fields: list[str]) -> bool:
    """ç…§åˆç‡90%ä»¥ä¸Šã‹ã¤å¿…é ˆé …ç›®ã«å•é¡Œãªã—ãªã‚‰True"""
    return pct >= 90 and not any(col in low_fields for col in REQUIRED_FIELDS)


def _match_key(row: dict) -> Optional[str]:
    """çªåˆã‚­ãƒ¼ã‚’ç”Ÿæˆã€‚å—çµ¦è€…è¨¼ç•ªå·å„ªå…ˆã€ãªã‘ã‚Œã°å§“å+ç”Ÿå¹´æœˆæ—¥"""
    cert = str(row.get("å—çµ¦è€…è¨¼ç•ªå·", "")).strip()
    if cert:
        return f"cert:{cert}"
    sei = str(row.get("åˆ©ç”¨è€…_å§“", "")).strip()
    mei = str(row.get("åˆ©ç”¨è€…_å", "")).strip()
    birth = str(row.get("ç”Ÿå¹´æœˆæ—¥", "")).strip()
    if sei and mei and birth:
        return f"name:{sei}|{mei}|{birth}"
    return None


def _name_key(row: dict) -> Optional[str]:
    """å§“åã®ã¿ã®çªåˆã‚­ãƒ¼ï¼ˆåˆ©ç”¨å¥‘ç´„æ›¸ãªã©ç”Ÿå¹´æœˆæ—¥ãŒãªã„ã‚±ãƒ¼ã‚¹ç”¨ï¼‰"""
    sei = str(row.get("åˆ©ç”¨è€…_å§“", "")).strip()
    mei = str(row.get("åˆ©ç”¨è€…_å", "")).strip()
    if sei and mei:
        return f"{sei}|{mei}"
    return None


def _merge_into(existing: dict, data: dict):
    """æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆç©ºæ¬„åŸ‹ã‚ãƒ»highå„ªå…ˆï¼‰"""
    existing_conf = existing.get("confidence", {})
    new_conf = data.get("confidence", {})
    for col in CSV_COLUMNS:
        new_val = str(data.get(col, "")).strip()
        old_val = str(existing.get(col, "")).strip()
        new_c = new_conf.get(col, "low")
        old_c = existing_conf.get(col, "low")
        if not old_val and new_val:
            existing[col] = new_val
            existing_conf[col] = new_c
        elif old_val and new_val:
            if new_c == "high" and old_c == "low":
                existing[col] = new_val
                existing_conf[col] = "high"
            elif len(new_val) > len(old_val) and new_c == old_c:
                existing[col] = new_val
    existing["confidence"] = existing_conf
    src = data.get("_source_file", "")
    if src and src not in existing.get("_source_files", []):
        existing.setdefault("_source_files", []).append(src)
        existing.setdefault("_source_types", []).append(data.get("_doc_type", "ä¸æ˜"))


def merge_records(data_list: list[dict]) -> list[dict]:
    """åŒä¸€äººç‰©ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’çªåˆã—ã€ç©ºæ¬„ã‚’ã§ãã‚‹ã ã‘åŸ‹ã‚ãŸãƒªã‚¹ãƒˆã‚’è¿”ã™

    2æ®µéšçªåˆ:
    1. _match_keyï¼ˆå—çµ¦è€…è¨¼ç•ªå· or å§“å+ç”Ÿå¹´æœˆæ—¥ï¼‰ã§ãƒãƒƒãƒ
    2. ãƒãƒƒãƒã—ãªã‹ã£ãŸåˆ†ã‚’ _name_keyï¼ˆå§“åã®ã¿ï¼‰ã§æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ ãƒãƒƒãƒ
    """
    groups: OrderedDict[str, dict] = OrderedDict()
    unmatched = []

    # ç¬¬1æ®µéš: å—çµ¦è€…è¨¼ç•ªå· or å§“å+ç”Ÿå¹´æœˆæ—¥
    for data in data_list:
        key = _match_key(data)
        if key is None:
            unmatched.append(data)
            continue

        if key not in groups:
            merged = {col: data.get(col, "") for col in CSV_COLUMNS}
            merged["confidence"] = dict(data.get("confidence", {}))
            merged["_source_files"] = [data.get("_source_file", "")]
            merged["_source_types"] = [data.get("_doc_type", "ä¸æ˜")]
            groups[key] = merged
        else:
            _merge_into(groups[key], data)

    # ç¬¬2æ®µéš: å§“åã®ã¿ã§ãƒãƒƒãƒï¼ˆåˆ©ç”¨å¥‘ç´„æ›¸ãªã©ç”Ÿå¹´æœˆæ—¥ãŒãªã„ã‚±ãƒ¼ã‚¹ï¼‰
    # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã®å§“åã‚­ãƒ¼ â†’ ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼ã®é€†å¼•ãè¾æ›¸ã‚’æ§‹ç¯‰
    name_to_group: dict[str, str] = {}
    for gkey, gdata in groups.items():
        nk = _name_key(gdata)
        if nk:
            name_to_group[nk] = gkey

    still_unmatched = []
    for data in unmatched:
        nk = _name_key(data)
        if nk and nk in name_to_group:
            _merge_into(groups[name_to_group[nk]], data)
        else:
            still_unmatched.append(data)

    return list(groups.values()) + still_unmatched


def build_dataframe(data_list: list[dict]) -> tuple[pd.DataFrame, list[dict]]:
    """æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã‹ã‚‰DataFrame+ä¿¡é ¼åº¦æƒ…å ±ã‚’æ§‹ç¯‰"""
    rows = []
    confidence_info = []
    for data in data_list:
        pct, label, low_fields = calc_confidence(data)
        row = {"åˆ¤å®š": label, "ç…§åˆç‡": f"{pct}%"}
        for col in CSV_COLUMNS:
            row[col] = data.get(col, "")
        rows.append(row)
        confidence_info.append({"pct": pct, "label": label, "low_fields": low_fields})
    display_cols = ["åˆ¤å®š", "ç…§åˆç‡"] + CSV_COLUMNS
    return pd.DataFrame(rows, columns=display_cols), confidence_info


def to_csv_bytes(df: pd.DataFrame) -> bytes:
    """DataFrameã‚’UTF-8 BOMä»˜ãCSVãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›"""
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


# =============================================================================
# Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº
# =============================================================================


def append_to_google_sheet(df: pd.DataFrame, spreadsheet_url: str, sheet_name: str) -> int:
    """Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æœ«å°¾ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜ã—ã€è¿½è¨˜ã—ãŸè¡Œæ•°ã‚’è¿”ã™"""
    import gspread

    creds_path = Path(__file__).parent / "credentials.json"
    if not creds_path.exists():
        raise FileNotFoundError(
            "credentials.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            "Google Cloud ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ "
            f"{creds_path} ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚"
        )

    gc = gspread.service_account(filename=str(creds_path))
    sh = gc.open_by_url(spreadsheet_url)
    worksheet = sh.worksheet(sheet_name)

    if not worksheet.get_all_values():
        worksheet.append_row(CSV_COLUMNS, value_input_option="USER_ENTERED")

    rows = df.fillna("").astype(str).values.tolist()
    worksheet.append_rows(rows, value_input_option="USER_ENTERED")
    return len(rows)


# =============================================================================
# ã‚¿ãƒ–é–‰ã˜é˜²æ­¢ JS
# =============================================================================

_GUARD_ON_JS = """
<script>
(function() {
    try { var w = window.parent.document ? window.parent : window; }
    catch(e) { var w = window; }
    function guard(e) { e.preventDefault(); e.returnValue = 'å‡¦ç†ä¸­ã§ã™ã€‚æœ¬å½“ã«ãƒšãƒ¼ã‚¸ã‚’é›¢ã‚Œã¾ã™ã‹ï¼Ÿ'; return e.returnValue; }
    w.addEventListener('beforeunload', guard);
    w.__streamlit_guard = guard;
})();
</script>
"""

_GUARD_OFF_JS = """
<script>
(function() {
    try { var w = window.parent.document ? window.parent : window; }
    catch(e) { var w = window; }
    if (w.__streamlit_guard) { w.removeEventListener('beforeunload', w.__streamlit_guard); delete w.__streamlit_guard; }
    w.onbeforeunload = null;
})();
</script>
"""


def inject_beforeunload_guard():
    st.components.v1.html(_GUARD_ON_JS, height=0)


def remove_beforeunload_guard():
    st.components.v1.html(_GUARD_OFF_JS, height=0)


# =============================================================================
# UI: â‘  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =============================================================================


def render_upload_section() -> list:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰UIã‚’æç”»ã—ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    st.header("â‘  æ›¸é¡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.caption(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã€ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠï¼ˆæœ€å¤§{MAX_FILES}æšã¾ã§ï¼‰")

    uploaded_files = st.file_uploader(
        "å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["jpg", "jpeg", "png", "pdf"],
        accept_multiple_files=True,
        label_visibility="collapsed",
    )

    if not uploaded_files:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã€ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return []

    if len(uploaded_files) > MAX_FILES:
        st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯{MAX_FILES}æšã¾ã§ã§ã™ã€‚ç¾åœ¨{len(uploaded_files)}æšé¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚")
        return []

    return uploaded_files


# =============================================================================
# UI: â‘¡ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
# =============================================================================


def render_file_list(uploaded_files: list):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æç”»"""
    n = len(uploaded_files)
    st.header(f"â‘¡ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆ{n}ä»¶ï¼‰")
    st.success(f"{n}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")

    with st.expander("ãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§", expanded=False):
        for i, uf in enumerate(uploaded_files):
            st.text(f"{i + 1}. {uf.name}")

    with st.expander("ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
        cols = st.columns(3)
        for i, uf in enumerate(uploaded_files):
            with cols[i % 3]:
                st.image(uf, caption=uf.name, use_container_width=True)


# =============================================================================
# UI: â‘¢ AIæŠ½å‡º
# =============================================================================


def _prepare_file(uf) -> tuple[str, bytes, Optional[str], Optional[str]]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ãƒ»åœ§ç¸®ã—ã¦APIå‘¼ã³å‡ºã—ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰

    Returns: (fname, image_bytes, image_base64, media_type)
        image_base64ãŒNoneã®å ´åˆã¯æº–å‚™å¤±æ•—
    """
    file_bytes = uf.read()
    fname = uf.name

    if fname.lower().endswith(".pdf"):
        image_bytes = convert_pdf_to_image(file_bytes)
        if image_bytes is None:
            st.warning(f"PDFå¤‰æ›å¤±æ•—: {fname}")
            return fname, file_bytes, None, None
        mtype = "image/png"
    else:
        image_bytes = file_bytes
        mtype = get_media_type(fname)

    compressed, comp_mtype = compress_image(image_bytes, mtype)
    image_base64 = base64.b64encode(compressed).decode("utf-8")
    return fname, image_bytes, image_base64, comp_mtype


def _call_api(fname: str, image_base64: str, media_type: str) -> tuple[str, Optional[dict]]:
    """APIå‘¼ã³å‡ºã—ã®ã¿ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰"""
    extracted = extract_with_anthropic(image_base64, media_type)
    return fname, extracted


MAX_PARALLEL = 3  # ä¸¦åˆ—APIå‘¼ã³å‡ºã—æ•°


def render_extraction_section(uploaded_files: list):
    """AIæŠ½å‡ºã®ãƒœã‚¿ãƒ³ã¨å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä¸¦åˆ—APIå‘¼ã³å‡ºã—å¯¾å¿œï¼‰"""
    st.header("â‘¢ AIã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")

    if not st.button("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹", type="primary", use_container_width=True):
        return

    st.session_state["processing"] = True
    inject_beforeunload_guard()

    total = len(uploaded_files)
    progress = st.progress(0, text=f"æŠ½å‡ºä¸­... 0/{total}ä»¶ å®Œäº†")
    start_time = time.time()

    # â”€â”€ ç¬¬1æ®µéš: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ»åœ§ç¸®ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã€é«˜é€Ÿï¼‰ â”€â”€
    prepared = []  # [(fname, image_bytes, image_base64, media_type), ...]
    for uf in uploaded_files:
        prepared.append(_prepare_file(uf))

    # â”€â”€ ç¬¬2æ®µéš: APIå‘¼ã³å‡ºã—ã‚’ä¸¦åˆ—å®Ÿè¡Œ â”€â”€
    all_images = {}
    api_results: dict[str, Optional[dict]] = {}  # fname â†’ extracted
    completed = 0
    lock = threading.Lock()

    # æº–å‚™æˆåŠŸã—ãŸã‚‚ã®ã®ã¿APIå‘¼ã³å‡ºã—
    api_tasks = [(fname, img_b64, mtype)
                 for fname, _img_bytes, img_b64, mtype in prepared
                 if img_b64 is not None]

    def _on_complete(fname: str):
        """å®Œäº†ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        nonlocal completed
        with lock:
            completed += 1
            c = completed

        elapsed = time.time() - start_time
        if c > 0 and c < total:
            avg_sec = elapsed / c
            remaining = avg_sec * (total - c)
            if remaining >= 60:
                eta = f"ï¼ˆæ®‹ã‚Šç´„{int(remaining // 60)}åˆ†{int(remaining % 60)}ç§’ï¼‰"
            else:
                eta = f"ï¼ˆæ®‹ã‚Šç´„{int(remaining)}ç§’ï¼‰"
        else:
            eta = ""
        progress.progress(c / total, text=f"æŠ½å‡ºä¸­... {c}/{total}ä»¶ å®Œäº† {eta}")

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_PARALLEL) as executor:
        futures = {}
        for fname, img_b64, mtype in api_tasks:
            future = executor.submit(_call_api, fname, img_b64, mtype)
            futures[future] = fname

        for future in concurrent.futures.as_completed(futures):
            fname, extracted = future.result()
            api_results[fname] = extracted
            _on_complete(fname)

    # æº–å‚™å¤±æ•—åˆ†ã‚‚ã‚«ã‚¦ãƒ³ãƒˆ
    for fname, _img_bytes, img_b64, _mtype in prepared:
        if img_b64 is None and fname not in api_results:
            api_results[fname] = None
            completed += 1
            progress.progress(
                min(completed / total, 1.0),
                text=f"æŠ½å‡ºä¸­... {completed}/{total}ä»¶ å®Œäº†",
            )

    # â”€â”€ ç¬¬3æ®µéš: çµæœã‚’å…ƒã®é †åºã§çµ„ã¿ç«‹ã¦ â”€â”€
    results = []
    for fname, image_bytes, _img_b64, _mtype in prepared:
        all_images[fname] = image_bytes
        extracted = api_results.get(fname)

        if extracted is not None:
            extracted["_source_file"] = fname
            extracted["_doc_type"] = extracted.get("æ›¸é¡ç¨®åˆ¥", "ä¸æ˜")
            results.append(extracted)
        else:
            if not fname.lower().endswith(".pdf"):
                st.warning(f"æŠ½å‡ºå¤±æ•—: {fname}")
            empty = {col: "" for col in CSV_COLUMNS}
            empty["_source_file"] = fname
            empty["_doc_type"] = "ä¸æ˜"
            results.append(empty)

    elapsed_total = time.time() - start_time
    if elapsed_total >= 60:
        time_str = f"{int(elapsed_total // 60)}åˆ†{int(elapsed_total % 60)}ç§’"
    else:
        time_str = f"{int(elapsed_total)}ç§’"
    progress.progress(1.0, text=f"å®Œäº†ï¼ {total}/{total}ä»¶ã‚’{time_str}ã§æŠ½å‡ºã—ã¾ã—ãŸã€‚")

    merged = merge_records(results)
    st.session_state["extracted_data"] = merged
    st.session_state["raw_count"] = len(results)
    st.session_state["all_images"] = all_images
    st.session_state["processing"] = False
    st.rerun()


# =============================================================================
# UI: ç”»åƒä»˜ããƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèªãƒ»ç·¨é›†
# =============================================================================


def _get_record_name(data: dict, data_idx: int) -> str:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ã®è¡¨ç¤ºåã‚’å–å¾—"""
    name = f"{data.get('åˆ©ç”¨è€…_å§“', '')} {data.get('åˆ©ç”¨è€…_å', '')}".strip()
    return name or f"ãƒ¬ã‚³ãƒ¼ãƒ‰{data_idx + 1}"


def _field_status(col_name: str, value: str, low_fields: list[str]) -> tuple[str, str]:
    """ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å…¥åŠ›ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’åˆ¤å®š

    Returns: (status_icon, help_text)
        ğŸ¤– = AIãŒè‡ªä¿¡ã‚ã‚Šã§å…¥åŠ›  âš ï¸ = AIå…¥åŠ›ã ãŒä¸é®®æ˜  âœï¸ = æœªå…¥åŠ›ï¼ˆæ‰‹å…¥åŠ›å¿…è¦ï¼‰
    """
    val = value.strip()
    if not val:
        is_required = col_name in REQUIRED_FIELDS
        req_label = "ï¼ˆå¿…é ˆï¼‰" if is_required else "ï¼ˆä»»æ„ï¼‰"
        return "âœï¸", f"æœªå…¥åŠ›{req_label} â€” æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„"
    elif col_name in low_fields:
        return "âš ï¸", "AIèª­å–ã ãŒä¸é®®æ˜ â€” ç¢ºèªãƒ»ä¿®æ­£ã—ã¦ãã ã•ã„"
    else:
        return "ğŸ¤–", "AIèª­å–ï¼ˆè‡ªä¿¡ã‚ã‚Šï¼‰"


def _render_review_card(
    item_idx: int, data_idx: int, data: dict,
    imgs: list, pct: int, low_fields: list[str],
    delete_checks: dict,
):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚«ãƒ¼ãƒ‰1ä»¶ã‚’æç”»ï¼ˆHuman In The Loopå¯¾å¿œï¼‰"""
    name = _get_record_name(data, data_idx)
    source_types = data.get("_source_types", [])

    if len(imgs) > 1:
        type_names = [t for t in source_types if t != "ä¸æ˜"] if source_types else []
        if type_names:
            merged_label = f"ï¼ˆ{'ï¼‹'.join(type_names)} ã‚’çªåˆï¼‰"
        else:
            merged_label = f"ï¼ˆæ›¸é¡{len(imgs)}æšã‚’çªåˆï¼‰"
    else:
        merged_label = ""

    # â”€â”€ ãƒ˜ãƒƒãƒ€ãƒ¼ â”€â”€
    if pct < 60:
        st.error(f"**{name}** â€” ç…§åˆç‡ {pct}%{merged_label}ã€€ä¸æ˜é …ç›®: {', '.join(low_fields)}")
    elif pct < 90:
        st.warning(f"**{name}** â€” ç…§åˆç‡ {pct}%{merged_label}ã€€ä¸æ˜é …ç›®: {', '.join(low_fields)}")
    else:
        st.success(f"**{name}** â€” ç…§åˆç‡ {pct}%{merged_label}")

    # â”€â”€ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚µãƒãƒªãƒ¼ï¼ˆå‡¡ä¾‹ï¼‹æ¦‚æ³ï¼‰ â”€â”€
    n_ai = sum(1 for c in CSV_COLUMNS if str(data.get(c, "")).strip() and c not in low_fields)
    n_warn = sum(1 for c in CSV_COLUMNS if str(data.get(c, "")).strip() and c in low_fields)
    n_empty = sum(1 for c in CSV_COLUMNS if not str(data.get(c, "")).strip())
    st.caption(
        f"ğŸ¤– AIå…¥åŠ›ï¼ˆè‡ªä¿¡ã‚ã‚Šï¼‰: {n_ai}ä»¶ã€€|ã€€"
        f"âš ï¸ AIå…¥åŠ›ï¼ˆè¦ç¢ºèªï¼‰: {n_warn}ä»¶ã€€|ã€€"
        f"âœï¸ æœªå…¥åŠ›ï¼ˆæ‰‹å…¥åŠ›ï¼‰: {n_empty}ä»¶"
    )

    col_img, col_form = st.columns([1, 2])

    # â”€â”€ ç”»åƒè¡¨ç¤º â”€â”€
    with col_img:
        if len(imgs) > 1:
            tab_labels = []
            for i, (fname, _img_bytes) in enumerate(imgs):
                doc_type = source_types[i] if i < len(source_types) else f"æ›¸é¡{i + 1}"
                tab_labels.append(f"{doc_type} ({fname})")
            img_tabs = st.tabs(tab_labels)
            for i, (fname, img_bytes) in enumerate(imgs):
                with img_tabs[i]:
                    st.image(img_bytes, caption=fname, use_container_width=True)
        else:
            st.image(imgs[0][1], caption=imgs[0][0], use_container_width=True)

    # â”€â”€ ãƒ•ã‚©ãƒ¼ãƒ ï¼ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã”ã¨ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºï¼‰ â”€â”€
    with col_form:
        form_cols = st.columns(3)
        for fi, col_name in enumerate(CSV_COLUMNS):
            with form_cols[fi % 3]:
                val = str(data.get(col_name, ""))
                icon, help_text = _field_status(col_name, val, low_fields)
                st.text_input(
                    f"{icon} {col_name}",
                    value=val,
                    key=f"review_{item_idx}_{col_name}",
                    help=help_text,
                )

        # â”€â”€ ç¢ºèªæ¸ˆã¿ / å‰Šé™¤ â”€â”€
        action_cols = st.columns(2)
        with action_cols[0]:
            st.checkbox(
                "âœ… å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ãŸ",
                key=f"confirmed_{item_idx}",
                value=data.get("_confirmed", False),
            )
        with action_cols[1]:
            delete_checks[item_idx] = st.checkbox(
                "ğŸ—‘ï¸ ã“ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã™ã‚‹",
                key=f"del_check_{item_idx}",
            )

    st.divider()


def _apply_review_changes(review_items: list, delete_checks: dict, data_list: list):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‚’ data_list ã«åæ˜ ã—ã€å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é™¤å»ã™ã‚‹"""
    del_indices = set()
    deleted_files = set()
    confirmed_count = 0

    for item_idx, (data_idx, _data, imgs, _pct, _lf) in enumerate(review_items):
        if delete_checks.get(item_idx):
            del_indices.add(data_idx)
            deleted_files.update(f for f, _ in imgs)
        else:
            for col_name in CSV_COLUMNS:
                val = st.session_state.get(f"review_{item_idx}_{col_name}", "")
                data_list[data_idx][col_name] = val
            if "confidence" in data_list[data_idx]:
                for col_name in CSV_COLUMNS:
                    data_list[data_idx]["confidence"][col_name] = "high"
            # ç¢ºèªæ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’ä¿å­˜
            is_confirmed = st.session_state.get(f"confirmed_{item_idx}", False)
            data_list[data_idx]["_confirmed"] = is_confirmed
            if is_confirmed:
                confirmed_count += 1

    if del_indices:
        for idx in sorted(del_indices, reverse=True):
            data_list.pop(idx)
        for fn in deleted_files:
            st.session_state["all_images"].pop(fn, None)

    st.session_state["extracted_data"] = data_list

    applied = len(review_items) - len(del_indices)
    msg_parts = []
    if applied:
        msg_parts.append(f"{applied}ä»¶ã‚’ä¿®æ­£åæ˜ ï¼ˆã†ã¡{confirmed_count}ä»¶ãŒç¢ºèªæ¸ˆã¿ï¼‰")
    if del_indices:
        msg_parts.append(f"{len(del_indices)}ä»¶ã‚’å‰Šé™¤")
    st.success("ãƒ»".join(msg_parts) + " ã—ã¾ã—ãŸã€‚")
    st.rerun()


def render_review_section():
    """ç”»åƒä»˜ããƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèªãƒ»ç·¨é›†UIï¼ˆHuman In The Loopï¼‰"""
    if "all_images" not in st.session_state or "extracted_data" not in st.session_state:
        return

    img_map = st.session_state["all_images"]
    data_list = st.session_state["extracted_data"]

    review_items = []
    for idx, data in enumerate(data_list):
        src_files = data.get("_source_files", [data.get("_source_file", "")])
        matching_imgs = [(f, img_map[f]) for f in src_files if f in img_map]
        if matching_imgs:
            pct, _label, low_fields = calc_confidence(data)
            review_items.append((idx, data, matching_imgs, pct, low_fields))

    if not review_items:
        return

    items_ok = [(ii, di, d, im, p, lf)
                for ii, (di, d, im, p, lf) in enumerate(review_items)
                if is_record_ok(p, lf)]
    items_ng = [(ii, di, d, im, p, lf)
                for ii, (di, d, im, p, lf) in enumerate(review_items)
                if not is_record_ok(p, lf)]

    # â”€â”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆã€Œä¸‹æ›¸ãã€ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤ºï¼‰ â”€â”€
    st.header("ğŸ“ AIã®ä¸‹æ›¸ã â€” ç¢ºèªãƒ»ä¿®æ­£")
    st.info(
        "ä»¥ä¸‹ã¯AIãŒç”»åƒã‹ã‚‰è‡ªå‹•èª­ã¿å–ã‚Šã—ãŸ**ä¸‹æ›¸ã**ã§ã™ã€‚\n\n"
        "å„é …ç›®ã®ã‚¢ã‚¤ã‚³ãƒ³ã§å…¥åŠ›å…ƒã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚\n"
        "- ğŸ¤– **AIå…¥åŠ›ï¼ˆè‡ªä¿¡ã‚ã‚Šï¼‰** â€” èª­ã¿å–ã‚Šç²¾åº¦ãŒé«˜ã„é …ç›®ã€‚å¿µã®ãŸã‚ç›®è¦–ç¢ºèªã‚’æ¨å¥¨\n"
        "- âš ï¸ **AIå…¥åŠ›ï¼ˆè¦ç¢ºèªï¼‰** â€” ä¸é®®æ˜ãƒ»æ¨æ¸¬ã‚’å«ã‚€é …ç›®ã€‚**å¿…ãšç¢ºèªã—ã¦ãã ã•ã„**\n"
        "- âœï¸ **æœªå…¥åŠ›** â€” æ›¸é¡ã‹ã‚‰èª­ã¿å–ã‚Œãªã‹ã£ãŸé …ç›®ã€‚**æ‰‹å…¥åŠ›ãŒå¿…è¦ã§ã™**\n\n"
        "ç¢ºèªãŒçµ‚ã‚ã£ãŸã‚‰ã€Œâœ… å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ãŸã€ã«ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚"
    )

    # â”€â”€ ç¢ºèªæ¸ˆã¿ã‚«ã‚¦ãƒ³ãƒˆ â”€â”€
    confirmed_count = sum(
        1 for ii, *_ in (items_ok + items_ng)
        if st.session_state.get(f"confirmed_{ii}", False)
    )
    total_count = len(review_items)
    st.progress(
        confirmed_count / total_count if total_count else 0,
        text=f"ç¢ºèªæ¸ˆã¿: {confirmed_count}/{total_count}ä»¶",
    )

    tab_ng, tab_ok = st.tabs([
        f"âš ï¸ è¦ç¢ºèªï¼ˆ{len(items_ng)}ä»¶ï¼‰",
        f"âœ… OKï¼ˆ{len(items_ok)}ä»¶ï¼‰",
    ])

    delete_checks: dict[int, bool] = {}

    with tab_ng:
        if items_ng:
            st.caption("âš ï¸ è¦ç¢ºèªé …ç›®ãŒã‚ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚ä¿®æ­£ãƒ»å‰Šé™¤å¾Œã€ä¸‹éƒ¨ã®ãƒœã‚¿ãƒ³ã§ä¸€æ‹¬åæ˜ ã•ã‚Œã¾ã™ã€‚")
            for item_idx, data_idx, data, imgs, pct, low_fields in items_ng:
                _render_review_card(item_idx, data_idx, data, imgs, pct, low_fields, delete_checks)
        else:
            st.info("è¦ç¢ºèªãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    with tab_ok:
        if items_ok:
            st.caption("ğŸ¤– AIèª­å–ã®ç²¾åº¦ãŒé«˜ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚å†…å®¹ã‚’ç›®è¦–ç¢ºèªã—ã¦ã€Œâœ… å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ãŸã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")
            for item_idx, data_idx, data, imgs, pct, low_fields in items_ok:
                _render_review_card(item_idx, data_idx, data, imgs, pct, low_fields, delete_checks)
        else:
            st.info("ç…§åˆç‡OKã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    if st.button("ã™ã¹ã¦ã®ä¿®æ­£ã‚’ã¾ã¨ã‚ã¦åæ˜ ", type="primary", use_container_width=True):
        _apply_review_changes(review_items, delete_checks, data_list)


# =============================================================================
# UI: â‘£ çµæœç¢ºèªãƒ»ç·¨é›† / â‘¤ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# =============================================================================


def render_results_section():
    """æŠ½å‡ºçµæœãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤ºãƒ»ç·¨é›†ãƒ»CSV/ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    if "extracted_data" not in st.session_state:
        return

    data_list = st.session_state["extracted_data"]
    raw_count = st.session_state.get("raw_count", len(data_list))

    df, conf_info = build_dataframe(data_list)
    ok_count = sum(1 for c in conf_info if c["label"] == "OK")
    review_count = len(conf_info) - ok_count

    # ç¢ºèªæ¸ˆã¿ã‚«ã‚¦ãƒ³ãƒˆ
    total_confirmed = sum(1 for d in data_list if d.get("_confirmed", False))
    total_unconfirmed = len(data_list) - total_confirmed

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    if raw_count != len(data_list):
        st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{raw_count}ä»¶ â†’ çªåˆå¾Œ {len(data_list)}ä»¶ï¼‰")
    else:
        st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{len(data_list)}ä»¶ï¼‰")

    # æœªç¢ºèªãƒ¬ã‚³ãƒ¼ãƒ‰è­¦å‘Š
    if total_unconfirmed > 0:
        st.warning(
            f"âš ï¸ **{total_unconfirmed}ä»¶ãŒæœªç¢ºèª**ã§ã™ã€‚"
            "ä¸Šã®ã€ŒğŸ“ AIã®ä¸‹æ›¸ãã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å†…å®¹ã‚’ç¢ºèªã—ã€"
            "ã€Œâœ… å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ãŸã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        )

    # ã‚µãƒãƒªãƒ¼
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("âœ… ç¢ºèªæ¸ˆã¿", f"{total_confirmed}ä»¶")
    col2.metric("âš ï¸ æœªç¢ºèª", f"{total_unconfirmed}ä»¶")
    avg_pct = sum(c["pct"] for c in conf_info) // len(conf_info) if conf_info else 0
    col3.metric("å¹³å‡ç…§åˆç‡", f"{avg_pct}%")
    col4.metric("åˆè¨ˆ", f"{len(data_list)}ä»¶")

    # ãƒ•ã‚£ãƒ«ã‚¿
    view_filter = st.radio("è¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿", ["ã™ã¹ã¦", "è¦ç¢ºèªã®ã¿", "OKã®ã¿"], horizontal=True)

    if view_filter == "è¦ç¢ºèªã®ã¿":
        mask = df["åˆ¤å®š"] != "OK"
        display_df = df[mask].reset_index(drop=True)
        display_conf = [c for c in conf_info if c["label"] != "OK"]
    elif view_filter == "OKã®ã¿":
        mask = df["åˆ¤å®š"] == "OK"
        display_df = df[mask].reset_index(drop=True)
        display_conf = [c for c in conf_info if c["label"] == "OK"]
    else:
        display_df = df
        display_conf = conf_info

    # è¦ç¢ºèªé …ç›®ã®è©³ç´°
    if display_conf and any(c["low_fields"] for c in display_conf):
        with st.expander("è¦ç¢ºèªé …ç›®ã®è©³ç´°", expanded=review_count > 0):
            for i, c in enumerate(display_conf):
                if c["low_fields"]:
                    row = display_df.iloc[i]
                    name = f"{row.get('åˆ©ç”¨è€…_å§“', '')} {row.get('åˆ©ç”¨è€…_å', '')}".strip() or f"è¡Œ{i + 1}"
                    st.markdown(f"**{name}** (ç…§åˆç‡ {c['pct']}%) â€” ä¸æ˜é …ç›®: {', '.join(c['low_fields'])}")

    # ç·¨é›†å¯èƒ½ãƒ†ãƒ¼ãƒ–ãƒ«
    st.caption("å„ã‚»ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç›´æ¥ä¿®æ­£ã§ãã¾ã™ã€‚ã€Œåˆ¤å®šã€ã€Œç…§åˆç‡ã€åˆ—ã¯å‡ºåŠ›ã«å«ã¾ã‚Œã¾ã›ã‚“ã€‚")
    edited_df = st.data_editor(
        display_df,
        use_container_width=True,
        num_rows="dynamic",
        disabled=["åˆ¤å®š", "ç…§åˆç‡"],
        key="data_editor",
    )

    export_df = edited_df[CSV_COLUMNS] if all(c in edited_df.columns for c in CSV_COLUMNS) else edited_df

    # â‘¤ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.header("â‘¤ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

    if total_unconfirmed > 0:
        st.caption(f"âš ï¸ {total_unconfirmed}ä»¶ãŒæœªç¢ºèªã®ã¾ã¾ã§ã™ã€‚å‡ºåŠ›å¾Œã‚‚å†…å®¹ã®æœ€çµ‚ç¢ºèªã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")

    st.download_button(
        label=f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ{len(export_df)}ä»¶ / ã†ã¡ç¢ºèªæ¸ˆã¿{total_confirmed}ä»¶ï¼‰",
        data=to_csv_bytes(export_df),
        file_name="åˆ©ç”¨è€…æƒ…å ±.csv",
        mime="text/csv",
        use_container_width=True,
    )

    # â‘¤' ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº
    st.header("â‘¤' ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸åæ˜ ")

    spreadsheet_url = st.text_input(
        "Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL",
        value=st.session_state.get("spreadsheet_url", ""),
        placeholder="https://docs.google.com/spreadsheets/d/xxxxx/edit",
    )
    sheet_name = st.text_input(
        "ã‚·ãƒ¼ãƒˆå",
        value=st.session_state.get("sheet_name", "ã‚·ãƒ¼ãƒˆ1"),
    )
    st.session_state["spreadsheet_url"] = spreadsheet_url
    st.session_state["sheet_name"] = sheet_name

    if st.button(
        f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã‚‹ï¼ˆ{len(export_df)}ä»¶ï¼‰",
        type="primary",
        use_container_width=True,
        disabled=not spreadsheet_url,
    ):
        try:
            with st.spinner("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ä¸­..."):
                count = append_to_google_sheet(export_df, spreadsheet_url, sheet_name)
            st.success(f"{count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
        except FileNotFoundError as e:
            st.error(str(e))
        except Exception as e:
            st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    remove_beforeunload_guard()


# =============================================================================
# ãƒ¡ã‚¤ãƒ³
# =============================================================================


def main():
    st.set_page_config(page_title="åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ„ãƒ¼ãƒ«", page_icon="ğŸ“‹", layout="wide")
    st.title("åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«")
    st.caption("éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ â€” å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æŠ½å‡ºãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

    if st.session_state.get("processing"):
        inject_beforeunload_guard()

    # â‘  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_files = render_upload_section()
    if not uploaded_files:
        return

    # â‘¡ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    render_file_list(uploaded_files)

    # â‘¢ AIæŠ½å‡º
    render_extraction_section(uploaded_files)

    # ç”»åƒä»˜ããƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèªãƒ»ç·¨é›†
    render_review_section()

    # â‘£â‘¤ çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    render_results_section()


if __name__ == "__main__":
    main()
