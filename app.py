"""
åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼‰
éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ - å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æŠ½å‡º
"""

import base64
import io
import json
import os
import re
import time
from collections import OrderedDict
from pathlib import Path
from typing import Optional

import pandas as pd
import streamlit as st
from dotenv import load_dotenv

# app.pyã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®.envã‚’æ˜ç¤ºçš„ã«èª­ã¿è¾¼ã‚€ï¼ˆã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒå¤‰æ•°ã‚ˆã‚Šå„ªå…ˆï¼‰
load_dotenv(Path(__file__).parent / ".env", override=True)

# =============================================================================
# å®šæ•°
# =============================================================================

CSV_COLUMNS = [
    "äº‹æ¥­",
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "åˆ©ç”¨è€…_ã›ã„",
    "åˆ©ç”¨è€…_ã‚ã„",
    "æ€§åˆ¥",
    "ç”Ÿå¹´æœˆæ—¥",
    "ä¿è­·è€…_å§“",
    "ä¿è­·è€…_å",
    "å¥‘ç´„æ—¥",
    "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—",
    "å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥",
    "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨",
    "éƒµä¾¿ç•ªå·",
    "éƒ½é“åºœçœŒ",
    "ä½æ‰€",
    "é›»è©±ç•ªå·",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹",
]

REQUIRED_FIELDS = [
    "äº‹æ¥­",
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "åˆ©ç”¨è€…_ã›ã„",
    "åˆ©ç”¨è€…_ã‚ã„",
    "æ€§åˆ¥",
    "å¥‘ç´„æ—¥",
    "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—",
    "å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥",
    "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨",
]

EXTRACTION_PROMPT = """ã‚ãªãŸã¯éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹ã®æ›¸é¡èª­ã¿å–ã‚Šå°‚é–€ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã¯ã€Œå—çµ¦è€…è¨¼ã€ã¾ãŸã¯ã€Œå¥‘ç´„æ›¸ã€ã§ã™ã€‚

ä»¥ä¸‹ã®é …ç›®ã‚’ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Šã€JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
èª­ã¿å–ã‚Œãªã„é …ç›®ã¯ç©ºæ–‡å­—("")ã¨ã—ã¦ãã ã•ã„ã€‚

å„é …ç›®ã«ã¤ã„ã¦ã€èª­ã¿å–ã‚Šã®ç¢ºä¿¡åº¦ã‚’ "confidence" ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
- "high": ã¯ã£ãã‚Šèª­ã¿å–ã‚ŒãŸ
- "low": æ–‡å­—ãŒä¸é®®æ˜ã€æ¨æ¸¬ãŒå«ã¾ã‚Œã‚‹ã€ã¾ãŸã¯è©²å½“é …ç›®ãŒæ›¸é¡ä¸Šã«è¦‹å½“ãŸã‚‰ãªã„ãŒæ¨å®šã—ãŸ

æŠ½å‡ºé …ç›®:
1. äº‹æ¥­ â€” ã‚µãƒ¼ãƒ“ã‚¹ç¨®åˆ¥ã€‚ã€Œè¨ˆç”»ç›¸è«‡æ”¯æ´ã€ã€Œéšœå®³å…ç›¸è«‡æ”¯æ´ã€ãªã©
2. åˆ©ç”¨è€…_å§“
3. åˆ©ç”¨è€…_å
4. åˆ©ç”¨è€…_ã›ã„ â€” å§“ã®ã²ã‚‰ãŒãªèª­ã¿
5. åˆ©ç”¨è€…_ã‚ã„ â€” åã®ã²ã‚‰ãŒãªèª­ã¿
6. æ€§åˆ¥ â€” ç”· or å¥³
7. ç”Ÿå¹´æœˆæ—¥ â€” ä¾‹: 1990å¹´01æœˆ15æ—¥
8. ä¿è­·è€…_å§“ â€” å…ç«¥ã®å ´åˆã®ä¿è­·è€…ã®å§“ï¼ˆè©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—ï¼‰
9. ä¿è­·è€…_å â€” å…ç«¥ã®å ´åˆã®ä¿è­·è€…ã®åï¼ˆè©²å½“ãªã‘ã‚Œã°ç©ºæ–‡å­—ï¼‰
10. å¥‘ç´„æ—¥ â€” åˆ©ç”¨é–‹å§‹æ—¥ã€‚ä¾‹: 2020å¹´08æœˆ01æ—¥
11. å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ— â€” ã€ŒéšœãŒã„ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ã€ã€Œåœ°åŸŸç›¸è«‡æ”¯æ´å—çµ¦è€…è¨¼ã€ã€ŒéšœãŒã„å…é€šæ‰€å—çµ¦è€…è¨¼ã€ã®ã„ãšã‚Œã‹
12. å—çµ¦è€…è¨¼ç•ªå· â€” 10æ¡ã®æ•°å­—
13. æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ â€” ä¾‹: 2023å¹´02æœˆ05æ—¥
14. æ”¯çµ¦æ±ºå®šæº€äº†æ—¥ â€” ä¾‹: 2024å¹´02æœˆ29æ—¥
15. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ â€” æ•°å­—ã®ã¿ï¼ˆä¾‹: 3ï¼‰
16. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨ â€” æ•°å­—ã®ã¿ï¼ˆä¾‹: 6ï¼‰
17. éƒµä¾¿ç•ªå· â€” ãƒã‚¤ãƒ•ãƒ³ãªã—7æ¡ã®æ•°å­—ã®ã¿ï¼ˆä¾‹: 8120011ï¼‰
18. éƒ½é“åºœçœŒ
19. ä½æ‰€ â€” éƒ½é“åºœçœŒã‚ˆã‚Šå¾Œã®éƒ¨åˆ†
20. é›»è©±ç•ªå· â€” ãƒã‚¤ãƒ•ãƒ³ä»˜ãã§OKï¼ˆä¾‹: 092-710-4570ï¼‰
21. ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹

å›ç­”ã¯JSONå½¢å¼ã®ã¿ã§ã€ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚ä»¥ä¸‹ã®å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„:
{
  "äº‹æ¥­": "",
  "åˆ©ç”¨è€…_å§“": "",
  "åˆ©ç”¨è€…_å": "",
  "åˆ©ç”¨è€…_ã›ã„": "",
  "åˆ©ç”¨è€…_ã‚ã„": "",
  "æ€§åˆ¥": "",
  "ç”Ÿå¹´æœˆæ—¥": "",
  "ä¿è­·è€…_å§“": "",
  "ä¿è­·è€…_å": "",
  "å¥‘ç´„æ—¥": "",
  "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—": "",
  "å—çµ¦è€…è¨¼ç•ªå·": "",
  "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥": "",
  "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨": "",
  "éƒµä¾¿ç•ªå·": "",
  "éƒ½é“åºœçœŒ": "",
  "ä½æ‰€": "",
  "é›»è©±ç•ªå·": "",
  "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹": "",
  "confidence": {
    "äº‹æ¥­": "high",
    "åˆ©ç”¨è€…_å§“": "high",
    "åˆ©ç”¨è€…_å": "high",
    "åˆ©ç”¨è€…_ã›ã„": "low",
    "åˆ©ç”¨è€…_ã‚ã„": "low",
    "æ€§åˆ¥": "high",
    "ç”Ÿå¹´æœˆæ—¥": "high",
    "ä¿è­·è€…_å§“": "low",
    "ä¿è­·è€…_å": "low",
    "å¥‘ç´„æ—¥": "high",
    "å—çµ¦è€…è¨¼ã‚¿ã‚¤ãƒ—": "high",
    "å—çµ¦è€…è¨¼ç•ªå·": "high",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥": "high",
    "æ”¯çµ¦æ±ºå®šæº€äº†æ—¥": "high",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_å½“åˆNã‹æœˆ": "low",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æº€äº†æœˆã‹ã‚‰Nã‹æœˆã”ã¨": "low",
    "éƒµä¾¿ç•ªå·": "high",
    "éƒ½é“åºœçœŒ": "high",
    "ä½æ‰€": "high",
    "é›»è©±ç•ªå·": "low",
    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹": "low"
  }
}
"""

MAX_FILES = 10
MAX_IMAGE_BYTES = 4_500_000  # base64å¤‰æ›å¾Œã«5MBä»¥å†…ã«åã¾ã‚‹ã‚ˆã†ä½™è£•ã‚’æŒãŸã›ã‚‹

# =============================================================================
# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆå–å¾—
# =============================================================================


def get_secret(key: str) -> str:
    """Streamlit Cloud ã® secrets â†’ .env ã®é †ã§ã‚­ãƒ¼ã‚’å–å¾—"""
    try:
        return st.secrets[key]
    except (KeyError, FileNotFoundError):
        return os.getenv(key, "")


# =============================================================================
# ç”»åƒå‡¦ç†
# =============================================================================


def compress_image(image_bytes: bytes, media_type: str) -> tuple[bytes, str]:
    """ç”»åƒãŒAPIã®ä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆã«ãƒªã‚µã‚¤ã‚ºãƒ»åœ§ç¸®ã™ã‚‹"""
    from PIL import Image

    if len(image_bytes) <= MAX_IMAGE_BYTES:
        return image_bytes, media_type

    img = Image.open(io.BytesIO(image_bytes))

    for quality in (85, 70, 50, 35):
        max_dim = 2048 if quality >= 70 else 1600
        if max(img.size) > max_dim:
            img.thumbnail((max_dim, max_dim), Image.LANCZOS)

        buf = io.BytesIO()
        rgb_img = img.convert("RGB") if img.mode != "RGB" else img
        rgb_img.save(buf, format="JPEG", quality=quality)
        result = buf.getvalue()
        if len(result) <= MAX_IMAGE_BYTES:
            return result, "image/jpeg"

    img.thumbnail((1200, 1200), Image.LANCZOS)
    buf = io.BytesIO()
    img.convert("RGB").save(buf, format="JPEG", quality=30)
    return buf.getvalue(), "image/jpeg"


def convert_pdf_to_image(pdf_bytes: bytes) -> Optional[bytes]:
    """PDFã®1ãƒšãƒ¼ã‚¸ç›®ã‚’ç”»åƒã«å¤‰æ›"""
    try:
        from pdf2image import convert_from_bytes

        images = convert_from_bytes(pdf_bytes, first_page=1, last_page=1, dpi=200)
        if images:
            buf = io.BytesIO()
            images[0].save(buf, format="PNG")
            return buf.getvalue()
    except ImportError:
        st.error(
            "pdf2imageãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            "ã¾ãŸã€Popplerã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚‚å¿…è¦ã§ã™ã€‚\n"
            "Windows: https://github.com/oschwartz10612/poppler-windows/releases\n"
            "Mac: `brew install poppler`\n"
            "Linux: `sudo apt-get install poppler-utils`"
        )
    except Exception as e:
        st.error(f"PDFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    return None


def get_media_type(filename: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ MIME ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
    ext = filename.lower().rsplit(".", 1)[-1]
    return "image/jpeg" if ext in ("jpg", "jpeg") else "image/png"


# =============================================================================
# AI æŠ½å‡º
# =============================================================================


def parse_json_response(text: str) -> Optional[dict]:
    """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    if "```" in text:
        start = text.find("```")
        end = text.rfind("```")
        if start != end:
            json_block = text[start:end]
            first_newline = json_block.find("\n")
            if first_newline != -1:
                json_block = json_block[first_newline + 1:]
            try:
                return json.loads(json_block.strip())
            except json.JSONDecodeError:
                pass

    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end > start:
        try:
            return json.loads(text[start:end + 1])
        except json.JSONDecodeError:
            pass

    st.error("AIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’JSONå½¢å¼ã§è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.code(text, language="text")
    return None


def _postprocess_extraction(result: dict) -> dict:
    """æŠ½å‡ºçµæœã®å¾Œå‡¦ç†ï¼ˆéƒµä¾¿ç•ªå·ãƒã‚¤ãƒ•ãƒ³é™¤å»ãªã©ï¼‰"""
    postal = str(result.get("éƒµä¾¿ç•ªå·", ""))
    if postal:
        result["éƒµä¾¿ç•ªå·"] = re.sub(r"[^\d]", "", postal)
    return result


def extract_with_anthropic(image_base64: str, media_type: str) -> Optional[dict]:
    """Anthropic Claude Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        import anthropic

        api_key = get_secret("ANTHROPIC_API_KEY")
        if not api_key or api_key.startswith("sk-ant-xxx"):
            st.error("ANTHROPIC_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = anthropic.Anthropic(api_key=api_key)
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image", "source": {"type": "base64", "media_type": media_type, "data": image_base64}},
                    {"type": "text", "text": EXTRACTION_PROMPT},
                ],
            }],
        )
        result = parse_json_response(message.content[0].text)
        return _postprocess_extraction(result) if result else None
    except Exception as e:
        st.error(f"Anthropic API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def extract_with_openai(image_base64: str, media_type: str) -> Optional[dict]:
    """OpenAI GPT-4 Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        from openai import OpenAI

        api_key = get_secret("OPENAI_API_KEY")
        if not api_key or api_key.startswith("sk-xxx"):
            st.error("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-4o",
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:{media_type};base64,{image_base64}"}},
                    {"type": "text", "text": EXTRACTION_PROMPT},
                ],
            }],
        )
        result = parse_json_response(response.choices[0].message.content)
        return _postprocess_extraction(result) if result else None
    except Exception as e:
        st.error(f"OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆä¿¡é ¼åº¦ãƒ»çªåˆãƒ»DataFrameæ§‹ç¯‰ï¼‰
# =============================================================================


def calc_confidence(data: dict) -> tuple[int, str, list[str]]:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ç…§åˆç‡(%)ãƒ»åˆ¤å®šãƒ©ãƒ™ãƒ«ãƒ»è¦ç¢ºèªé …ç›®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    confidence_map = data.get("confidence", {})
    total_weight = 0
    earned = 0.0
    low_fields = []

    for col in CSV_COLUMNS:
        weight = 2 if col in REQUIRED_FIELDS else 1
        total_weight += weight
        val = str(data.get(col, "")).strip()
        ai_conf = confidence_map.get(col, "high" if val else "low")

        if not val:
            low_fields.append(col)
        elif ai_conf == "low":
            earned += weight * 0.5
            low_fields.append(col)
        else:
            earned += weight

    pct = int(earned / total_weight * 100) if total_weight else 0

    if pct >= 90 and not any(col in low_fields for col in REQUIRED_FIELDS):
        label = "OK"
    elif pct >= 60:
        label = "è¦ç¢ºèª"
    else:
        label = "è¦ç¢ºèª(ä½)"

    return pct, label, low_fields


def is_record_ok(pct: int, low_fields: list[str]) -> bool:
    """ç…§åˆç‡90%ä»¥ä¸Šã‹ã¤å¿…é ˆé …ç›®ã«å•é¡Œãªã—ãªã‚‰True"""
    return pct >= 90 and not any(col in low_fields for col in REQUIRED_FIELDS)


def _match_key(row: dict) -> Optional[str]:
    """çªåˆã‚­ãƒ¼ã‚’ç”Ÿæˆã€‚å—çµ¦è€…è¨¼ç•ªå·å„ªå…ˆã€ãªã‘ã‚Œã°å§“å+ç”Ÿå¹´æœˆæ—¥"""
    cert = str(row.get("å—çµ¦è€…è¨¼ç•ªå·", "")).strip()
    if cert:
        return f"cert:{cert}"
    sei = str(row.get("åˆ©ç”¨è€…_å§“", "")).strip()
    mei = str(row.get("åˆ©ç”¨è€…_å", "")).strip()
    birth = str(row.get("ç”Ÿå¹´æœˆæ—¥", "")).strip()
    if sei and mei and birth:
        return f"name:{sei}|{mei}|{birth}"
    return None


def merge_records(data_list: list[dict]) -> list[dict]:
    """åŒä¸€äººç‰©ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’çªåˆã—ã€ç©ºæ¬„ã‚’ã§ãã‚‹ã ã‘åŸ‹ã‚ãŸãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    groups: OrderedDict[str, dict] = OrderedDict()
    unmatched = []

    for data in data_list:
        key = _match_key(data)
        if key is None:
            unmatched.append(data)
            continue

        if key not in groups:
            merged = {col: data.get(col, "") for col in CSV_COLUMNS}
            merged["confidence"] = dict(data.get("confidence", {}))
            merged["_source_files"] = [data.get("_source_file", "")]
            groups[key] = merged
        else:
            existing = groups[key]
            existing_conf = existing.get("confidence", {})
            new_conf = data.get("confidence", {})
            for col in CSV_COLUMNS:
                new_val = str(data.get(col, "")).strip()
                old_val = str(existing.get(col, "")).strip()
                new_c = new_conf.get(col, "low")
                old_c = existing_conf.get(col, "low")
                if not old_val and new_val:
                    existing[col] = new_val
                    existing_conf[col] = new_c
                elif old_val and new_val:
                    if new_c == "high" and old_c == "low":
                        existing[col] = new_val
                        existing_conf[col] = "high"
                    elif len(new_val) > len(old_val) and new_c == old_c:
                        existing[col] = new_val
            existing["confidence"] = existing_conf
            src = data.get("_source_file", "")
            if src and src not in existing.get("_source_files", []):
                existing.setdefault("_source_files", []).append(src)

    return list(groups.values()) + unmatched


def build_dataframe(data_list: list[dict]) -> tuple[pd.DataFrame, list[dict]]:
    """æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã‹ã‚‰DataFrame+ä¿¡é ¼åº¦æƒ…å ±ã‚’æ§‹ç¯‰"""
    rows = []
    confidence_info = []
    for data in data_list:
        pct, label, low_fields = calc_confidence(data)
        row = {"åˆ¤å®š": label, "ç…§åˆç‡": f"{pct}%"}
        for col in CSV_COLUMNS:
            row[col] = data.get(col, "")
        rows.append(row)
        confidence_info.append({"pct": pct, "label": label, "low_fields": low_fields})
    display_cols = ["åˆ¤å®š", "ç…§åˆç‡"] + CSV_COLUMNS
    return pd.DataFrame(rows, columns=display_cols), confidence_info


def to_csv_bytes(df: pd.DataFrame) -> bytes:
    """DataFrameã‚’UTF-8 BOMä»˜ãCSVãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›"""
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


# =============================================================================
# Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº
# =============================================================================


def append_to_google_sheet(df: pd.DataFrame, spreadsheet_url: str, sheet_name: str) -> int:
    """Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æœ«å°¾ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜ã—ã€è¿½è¨˜ã—ãŸè¡Œæ•°ã‚’è¿”ã™"""
    import gspread

    creds_path = Path(__file__).parent / "credentials.json"
    if not creds_path.exists():
        raise FileNotFoundError(
            "credentials.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            "Google Cloud ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ "
            f"{creds_path} ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚"
        )

    gc = gspread.service_account(filename=str(creds_path))
    sh = gc.open_by_url(spreadsheet_url)
    worksheet = sh.worksheet(sheet_name)

    if not worksheet.get_all_values():
        worksheet.append_row(CSV_COLUMNS, value_input_option="USER_ENTERED")

    rows = df.fillna("").astype(str).values.tolist()
    worksheet.append_rows(rows, value_input_option="USER_ENTERED")
    return len(rows)


# =============================================================================
# ã‚¿ãƒ–é–‰ã˜é˜²æ­¢ JS
# =============================================================================

_GUARD_ON_JS = """
<script>
(function() {
    try { var w = window.parent.document ? window.parent : window; }
    catch(e) { var w = window; }
    function guard(e) { e.preventDefault(); e.returnValue = 'å‡¦ç†ä¸­ã§ã™ã€‚æœ¬å½“ã«ãƒšãƒ¼ã‚¸ã‚’é›¢ã‚Œã¾ã™ã‹ï¼Ÿ'; return e.returnValue; }
    w.addEventListener('beforeunload', guard);
    w.__streamlit_guard = guard;
})();
</script>
"""

_GUARD_OFF_JS = """
<script>
(function() {
    try { var w = window.parent.document ? window.parent : window; }
    catch(e) { var w = window; }
    if (w.__streamlit_guard) { w.removeEventListener('beforeunload', w.__streamlit_guard); delete w.__streamlit_guard; }
    w.onbeforeunload = null;
})();
</script>
"""


def inject_beforeunload_guard():
    st.components.v1.html(_GUARD_ON_JS, height=0)


def remove_beforeunload_guard():
    st.components.v1.html(_GUARD_OFF_JS, height=0)


# =============================================================================
# UI: â‘  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =============================================================================


def render_upload_section() -> list:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰UIã‚’æç”»ã—ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    st.header("â‘  æ›¸é¡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.caption(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã€ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠï¼ˆæœ€å¤§{MAX_FILES}æšã¾ã§ï¼‰")

    uploaded_files = st.file_uploader(
        "å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["jpg", "jpeg", "png", "pdf"],
        accept_multiple_files=True,
        label_visibility="collapsed",
    )

    if not uploaded_files:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã€ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return []

    if len(uploaded_files) > MAX_FILES:
        st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯{MAX_FILES}æšã¾ã§ã§ã™ã€‚ç¾åœ¨{len(uploaded_files)}æšé¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚")
        return []

    return uploaded_files


# =============================================================================
# UI: â‘¡ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
# =============================================================================


def render_file_list(uploaded_files: list):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æç”»"""
    n = len(uploaded_files)
    st.header(f"â‘¡ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆ{n}ä»¶ï¼‰")
    st.success(f"{n}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")

    with st.expander("ãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§", expanded=False):
        for i, uf in enumerate(uploaded_files):
            st.text(f"{i + 1}. {uf.name}")

    with st.expander("ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
        cols = st.columns(3)
        for i, uf in enumerate(uploaded_files):
            with cols[i % 3]:
                st.image(uf, caption=uf.name, use_container_width=True)


# =============================================================================
# UI: â‘¢ AIæŠ½å‡º
# =============================================================================


def _extract_single_file(uf) -> tuple[Optional[dict], bytes]:
    """1ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”»åƒèª­ã¿è¾¼ã¿â†’åœ§ç¸®â†’APIæŠ½å‡ºã‚’è¡Œã„ (æŠ½å‡ºçµæœ, ç”»åƒbytes) ã‚’è¿”ã™"""
    file_bytes = uf.read()
    fname = uf.name

    if fname.lower().endswith(".pdf"):
        image_bytes = convert_pdf_to_image(file_bytes)
        if image_bytes is None:
            st.warning(f"PDFå¤‰æ›å¤±æ•—: {fname}")
            return None, file_bytes
        mtype = "image/png"
    else:
        image_bytes = file_bytes
        mtype = get_media_type(fname)

    compressed, comp_mtype = compress_image(image_bytes, mtype)
    image_base64 = base64.b64encode(compressed).decode("utf-8")
    extracted = extract_with_anthropic(image_base64, comp_mtype)

    return extracted, image_bytes


def render_extraction_section(uploaded_files: list):
    """AIæŠ½å‡ºã®ãƒœã‚¿ãƒ³ã¨å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯"""
    st.header("â‘¢ AIã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")

    if not st.button("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹", type="primary", use_container_width=True):
        return

    st.session_state["processing"] = True
    inject_beforeunload_guard()

    results = []
    all_images = {}
    total = len(uploaded_files)
    progress = st.progress(0, text=f"æŠ½å‡ºä¸­... 0/{total}ä»¶ å®Œäº†")
    start_time = time.time()

    for i, uf in enumerate(uploaded_files):
        # æ®‹ã‚Šæ™‚é–“ã®æ¨å®š
        if i > 0:
            elapsed = time.time() - start_time
            avg_sec = elapsed / i
            remaining = avg_sec * (total - i)
            if remaining >= 60:
                eta = f"ï¼ˆæ®‹ã‚Šç´„{int(remaining // 60)}åˆ†{int(remaining % 60)}ç§’ï¼‰"
            else:
                eta = f"ï¼ˆæ®‹ã‚Šç´„{int(remaining)}ç§’ï¼‰"
        else:
            eta = ""
        progress.progress(i / total, text=f"æŠ½å‡ºä¸­... {i}/{total}ä»¶ å®Œäº† {eta}")

        extracted, image_bytes = _extract_single_file(uf)
        fname = uf.name

        if extracted is not None:
            extracted["_source_file"] = fname
            results.append(extracted)
        else:
            if not fname.lower().endswith(".pdf"):
                st.warning(f"æŠ½å‡ºå¤±æ•—: {fname}")
            empty = {col: "" for col in CSV_COLUMNS}
            empty["_source_file"] = fname
            results.append(empty)

        all_images[fname] = image_bytes

    elapsed_total = time.time() - start_time
    if elapsed_total >= 60:
        time_str = f"{int(elapsed_total // 60)}åˆ†{int(elapsed_total % 60)}ç§’"
    else:
        time_str = f"{int(elapsed_total)}ç§’"
    progress.progress(1.0, text=f"å®Œäº†ï¼ {total}/{total}ä»¶ã‚’{time_str}ã§æŠ½å‡ºã—ã¾ã—ãŸã€‚")

    merged = merge_records(results)
    st.session_state["extracted_data"] = merged
    st.session_state["raw_count"] = len(results)
    st.session_state["all_images"] = all_images
    st.session_state["processing"] = False
    st.rerun()


# =============================================================================
# UI: ç”»åƒä»˜ããƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèªãƒ»ç·¨é›†
# =============================================================================


def _get_record_name(data: dict, data_idx: int) -> str:
    """ãƒ¬ã‚³ãƒ¼ãƒ‰ã®è¡¨ç¤ºåã‚’å–å¾—"""
    name = f"{data.get('åˆ©ç”¨è€…_å§“', '')} {data.get('åˆ©ç”¨è€…_å', '')}".strip()
    return name or f"ãƒ¬ã‚³ãƒ¼ãƒ‰{data_idx + 1}"


def _render_review_card(
    item_idx: int, data_idx: int, data: dict,
    imgs: list, pct: int, low_fields: list[str],
    delete_checks: dict,
):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚«ãƒ¼ãƒ‰1ä»¶ã‚’æç”»"""
    name = _get_record_name(data, data_idx)
    merged_label = f"ï¼ˆæ›¸é¡{len(imgs)}æšã‚’çªåˆï¼‰" if len(imgs) > 1 else ""

    if pct < 60:
        st.error(f"**{name}** â€” ç…§åˆç‡ {pct}%{merged_label}ã€€ä¸æ˜é …ç›®: {', '.join(low_fields)}")
    elif pct < 90:
        st.warning(f"**{name}** â€” ç…§åˆç‡ {pct}%{merged_label}ã€€ä¸æ˜é …ç›®: {', '.join(low_fields)}")
    else:
        st.success(f"**{name}** â€” ç…§åˆç‡ {pct}%{merged_label}")

    col_img, col_form = st.columns([1, 2])

    with col_img:
        if len(imgs) > 1:
            img_tabs = st.tabs([f"æ›¸é¡{i + 1}" for i in range(len(imgs))])
            for i, (fname, img_bytes) in enumerate(imgs):
                with img_tabs[i]:
                    st.image(img_bytes, caption=fname, use_container_width=True)
        else:
            st.image(imgs[0][1], caption=imgs[0][0], use_container_width=True)

    with col_form:
        form_cols = st.columns(3)
        for fi, col_name in enumerate(CSV_COLUMNS):
            with form_cols[fi % 3]:
                is_low = col_name in low_fields
                label_suffix = " âš " if is_low else ""
                st.text_input(
                    f"{col_name}{label_suffix}",
                    value=str(data.get(col_name, "")),
                    key=f"review_{item_idx}_{col_name}",
                )

        delete_checks[item_idx] = st.checkbox(
            "ã“ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã™ã‚‹",
            key=f"del_check_{item_idx}",
        )

    st.divider()


def _apply_review_changes(review_items: list, delete_checks: dict, data_list: list):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‚’ data_list ã«åæ˜ ã—ã€å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é™¤å»ã™ã‚‹"""
    del_indices = set()
    deleted_files = set()

    for item_idx, (data_idx, _data, imgs, _pct, _lf) in enumerate(review_items):
        if delete_checks.get(item_idx):
            del_indices.add(data_idx)
            deleted_files.update(f for f, _ in imgs)
        else:
            for col_name in CSV_COLUMNS:
                val = st.session_state.get(f"review_{item_idx}_{col_name}", "")
                data_list[data_idx][col_name] = val
            if "confidence" in data_list[data_idx]:
                for col_name in CSV_COLUMNS:
                    data_list[data_idx]["confidence"][col_name] = "high"

    if del_indices:
        for idx in sorted(del_indices, reverse=True):
            data_list.pop(idx)
        for fn in deleted_files:
            st.session_state["all_images"].pop(fn, None)

    st.session_state["extracted_data"] = data_list

    applied = len(review_items) - len(del_indices)
    msg_parts = []
    if applied:
        msg_parts.append(f"{applied}ä»¶ã‚’ä¿®æ­£åæ˜ ")
    if del_indices:
        msg_parts.append(f"{len(del_indices)}ä»¶ã‚’å‰Šé™¤")
    st.success("ãƒ»".join(msg_parts) + " ã—ã¾ã—ãŸã€‚")
    st.rerun()


def render_review_section():
    """ç”»åƒä»˜ããƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèªãƒ»ç·¨é›†UI"""
    if "all_images" not in st.session_state or "extracted_data" not in st.session_state:
        return

    img_map = st.session_state["all_images"]
    data_list = st.session_state["extracted_data"]

    review_items = []
    for idx, data in enumerate(data_list):
        src_files = data.get("_source_files", [data.get("_source_file", "")])
        matching_imgs = [(f, img_map[f]) for f in src_files if f in img_map]
        if matching_imgs:
            pct, _label, low_fields = calc_confidence(data)
            review_items.append((idx, data, matching_imgs, pct, low_fields))

    if not review_items:
        return

    items_ok = [(ii, di, d, im, p, lf)
                for ii, (di, d, im, p, lf) in enumerate(review_items)
                if is_record_ok(p, lf)]
    items_ng = [(ii, di, d, im, p, lf)
                for ii, (di, d, im, p, lf) in enumerate(review_items)
                if not is_record_ok(p, lf)]

    st.header("ç”»åƒä»˜ããƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèª")
    tab_ng, tab_ok = st.tabs([
        f"è¦ç¢ºèªï¼ˆ{len(items_ng)}ä»¶ï¼‰",
        f"OKï¼ˆ{len(items_ok)}ä»¶ï¼‰",
    ])

    delete_checks: dict[int, bool] = {}

    with tab_ng:
        if items_ng:
            st.caption("ä¿®æ­£ãƒ»å‰Šé™¤é¸æŠã—ãŸå¾Œã€ä¸‹éƒ¨ã®ã€Œã™ã¹ã¦ã®ä¿®æ­£ã‚’ã¾ã¨ã‚ã¦åæ˜ ã€ã§ä¸€æ‹¬åæ˜ ã•ã‚Œã¾ã™ã€‚")
            for item_idx, data_idx, data, imgs, pct, low_fields in items_ng:
                _render_review_card(item_idx, data_idx, data, imgs, pct, low_fields, delete_checks)
        else:
            st.info("è¦ç¢ºèªãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    with tab_ok:
        if items_ok:
            st.caption("ç…§åˆç‡ãŒé«˜ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚å†…å®¹ã«å•é¡ŒãŒãªã‘ã‚Œã°ãã®ã¾ã¾ã§OKã§ã™ã€‚")
            for item_idx, data_idx, data, imgs, pct, low_fields in items_ok:
                _render_review_card(item_idx, data_idx, data, imgs, pct, low_fields, delete_checks)
        else:
            st.info("ç…§åˆç‡OKã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    if st.button("ã™ã¹ã¦ã®ä¿®æ­£ã‚’ã¾ã¨ã‚ã¦åæ˜ ", type="primary", use_container_width=True):
        _apply_review_changes(review_items, delete_checks, data_list)


# =============================================================================
# UI: â‘£ çµæœç¢ºèªãƒ»ç·¨é›† / â‘¤ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# =============================================================================


def render_results_section():
    """æŠ½å‡ºçµæœãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤ºãƒ»ç·¨é›†ãƒ»CSV/ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    if "extracted_data" not in st.session_state:
        return

    data_list = st.session_state["extracted_data"]
    raw_count = st.session_state.get("raw_count", len(data_list))

    df, conf_info = build_dataframe(data_list)
    ok_count = sum(1 for c in conf_info if c["label"] == "OK")
    review_count = len(conf_info) - ok_count

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    if raw_count != len(data_list):
        st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{raw_count}ä»¶ â†’ çªåˆå¾Œ {len(data_list)}ä»¶ï¼‰")
    else:
        st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{len(data_list)}ä»¶ï¼‰")

    # ã‚µãƒãƒªãƒ¼
    col1, col2, col3 = st.columns(3)
    col1.metric("OK (ç¢ºèªä¸è¦)", f"{ok_count}ä»¶")
    col2.metric("è¦ç¢ºèª", f"{review_count}ä»¶")
    avg_pct = sum(c["pct"] for c in conf_info) // len(conf_info) if conf_info else 0
    col3.metric("å¹³å‡ç…§åˆç‡", f"{avg_pct}%")

    # ãƒ•ã‚£ãƒ«ã‚¿
    view_filter = st.radio("è¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿", ["ã™ã¹ã¦", "è¦ç¢ºèªã®ã¿", "OKã®ã¿"], horizontal=True)

    if view_filter == "è¦ç¢ºèªã®ã¿":
        mask = df["åˆ¤å®š"] != "OK"
        display_df = df[mask].reset_index(drop=True)
        display_conf = [c for c in conf_info if c["label"] != "OK"]
    elif view_filter == "OKã®ã¿":
        mask = df["åˆ¤å®š"] == "OK"
        display_df = df[mask].reset_index(drop=True)
        display_conf = [c for c in conf_info if c["label"] == "OK"]
    else:
        display_df = df
        display_conf = conf_info

    # è¦ç¢ºèªé …ç›®ã®è©³ç´°
    if display_conf and any(c["low_fields"] for c in display_conf):
        with st.expander("è¦ç¢ºèªé …ç›®ã®è©³ç´°", expanded=review_count > 0):
            for i, c in enumerate(display_conf):
                if c["low_fields"]:
                    row = display_df.iloc[i]
                    name = f"{row.get('åˆ©ç”¨è€…_å§“', '')} {row.get('åˆ©ç”¨è€…_å', '')}".strip() or f"è¡Œ{i + 1}"
                    st.markdown(f"**{name}** (ç…§åˆç‡ {c['pct']}%) â€” ä¸æ˜é …ç›®: {', '.join(c['low_fields'])}")

    # ç·¨é›†å¯èƒ½ãƒ†ãƒ¼ãƒ–ãƒ«
    st.caption("å„ã‚»ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç›´æ¥ä¿®æ­£ã§ãã¾ã™ã€‚ã€Œåˆ¤å®šã€ã€Œç…§åˆç‡ã€åˆ—ã¯å‡ºåŠ›ã«å«ã¾ã‚Œã¾ã›ã‚“ã€‚")
    edited_df = st.data_editor(
        display_df,
        use_container_width=True,
        num_rows="dynamic",
        disabled=["åˆ¤å®š", "ç…§åˆç‡"],
        key="data_editor",
    )

    export_df = edited_df[CSV_COLUMNS] if all(c in edited_df.columns for c in CSV_COLUMNS) else edited_df

    # â‘¤ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.header("â‘¤ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    st.download_button(
        label=f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ{len(export_df)}ä»¶ï¼‰",
        data=to_csv_bytes(export_df),
        file_name="åˆ©ç”¨è€…æƒ…å ±.csv",
        mime="text/csv",
        use_container_width=True,
    )

    # â‘¤' ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº
    st.header("â‘¤' ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸åæ˜ ")

    spreadsheet_url = st.text_input(
        "Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL",
        value=st.session_state.get("spreadsheet_url", ""),
        placeholder="https://docs.google.com/spreadsheets/d/xxxxx/edit",
    )
    sheet_name = st.text_input(
        "ã‚·ãƒ¼ãƒˆå",
        value=st.session_state.get("sheet_name", "ã‚·ãƒ¼ãƒˆ1"),
    )
    st.session_state["spreadsheet_url"] = spreadsheet_url
    st.session_state["sheet_name"] = sheet_name

    if st.button(
        f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã‚‹ï¼ˆ{len(export_df)}ä»¶ï¼‰",
        type="primary",
        use_container_width=True,
        disabled=not spreadsheet_url,
    ):
        try:
            with st.spinner("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ä¸­..."):
                count = append_to_google_sheet(export_df, spreadsheet_url, sheet_name)
            st.success(f"{count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
        except FileNotFoundError as e:
            st.error(str(e))
        except Exception as e:
            st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    remove_beforeunload_guard()


# =============================================================================
# ãƒ¡ã‚¤ãƒ³
# =============================================================================


def main():
    st.set_page_config(page_title="åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ„ãƒ¼ãƒ«", page_icon="ğŸ“‹", layout="wide")
    st.title("åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«")
    st.caption("éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ â€” å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æŠ½å‡ºãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

    if st.session_state.get("processing"):
        inject_beforeunload_guard()

    # â‘  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_files = render_upload_section()
    if not uploaded_files:
        return

    # â‘¡ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    render_file_list(uploaded_files)

    # â‘¢ AIæŠ½å‡º
    render_extraction_section(uploaded_files)

    # ç”»åƒä»˜ããƒ¬ã‚³ãƒ¼ãƒ‰ç¢ºèªãƒ»ç·¨é›†
    render_review_section()

    # â‘£â‘¤ çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    render_results_section()


if __name__ == "__main__":
    main()
