"""
åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼‰
éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ - å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æŠ½å‡º
"""

import base64
import io
import json
import os
from pathlib import Path
from typing import Optional

import pandas as pd
import streamlit as st
from dotenv import load_dotenv

# app.pyã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®.envã‚’æ˜ç¤ºçš„ã«èª­ã¿è¾¼ã‚€ï¼ˆã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒå¤‰æ•°ã‚ˆã‚Šå„ªå…ˆï¼‰
load_dotenv(Path(__file__).parent / ".env", override=True)

# --- å®šæ•°å®šç¾© ---

CSV_COLUMNS = [
    "ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å",
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)",
    "åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)",
    "æ€§åˆ¥",
    "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ",
    "éƒµä¾¿ç•ªå·",
    "éƒ½é“åºœçœŒ",
    "ä½æ‰€",
]

EXTRACTION_PROMPT = """ã‚ãªãŸã¯éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹ã®æ›¸é¡èª­ã¿å–ã‚Šå°‚é–€ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã¯ã€Œå—çµ¦è€…è¨¼ã€ã¾ãŸã¯ã€Œå¥‘ç´„æ›¸ã€ã§ã™ã€‚

ä»¥ä¸‹ã®é …ç›®ã‚’ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Šã€JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
èª­ã¿å–ã‚Œãªã„é …ç›®ã¯ç©ºæ–‡å­—("")ã¨ã—ã¦ãã ã•ã„ã€‚

å„é …ç›®ã«ã¤ã„ã¦ã€èª­ã¿å–ã‚Šã®ç¢ºä¿¡åº¦ã‚’ "confidence" ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
- "high": ã¯ã£ãã‚Šèª­ã¿å–ã‚ŒãŸ
- "low": æ–‡å­—ãŒä¸é®®æ˜ã€æ¨æ¸¬ãŒå«ã¾ã‚Œã‚‹ã€ã¾ãŸã¯è©²å½“é …ç›®ãŒæ›¸é¡ä¸Šã«è¦‹å½“ãŸã‚‰ãªã„ãŒæ¨å®šã—ãŸ

æŠ½å‡ºé …ç›®:
1. ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å
2. åˆ©ç”¨è€…_å§“
3. åˆ©ç”¨è€…_å
4. åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)
5. åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)
6. æ€§åˆ¥ (ç”· or å¥³)
7. ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥) ä¾‹: 1990å¹´01æœˆ15æ—¥
8. éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·
9. æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥) ä¾‹: 2024å¹´04æœˆ01æ—¥
10. æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥) ä¾‹: 2025å¹´03æœˆ31æ—¥
11. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°
12. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ
13. éƒµä¾¿ç•ªå· (ä¾‹: 123-4567)
14. éƒ½é“åºœçœŒ
15. ä½æ‰€ (éƒ½é“åºœçœŒã‚ˆã‚Šå¾Œã®éƒ¨åˆ†)

å›ç­”ã¯JSONå½¢å¼ã®ã¿ã§ã€ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚ä»¥ä¸‹ã®å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„:
{
  "ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å": "",
  "åˆ©ç”¨è€…_å§“": "",
  "åˆ©ç”¨è€…_å": "",
  "åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)": "",
  "åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)": "",
  "æ€§åˆ¥": "",
  "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "",
  "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·": "",
  "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "",
  "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ": "",
  "éƒµä¾¿ç•ªå·": "",
  "éƒ½é“åºœçœŒ": "",
  "ä½æ‰€": "",
  "confidence": {
    "ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å": "high",
    "åˆ©ç”¨è€…_å§“": "high",
    "åˆ©ç”¨è€…_å": "high",
    "åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)": "low",
    "åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)": "low",
    "æ€§åˆ¥": "high",
    "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "high",
    "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·": "high",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "high",
    "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "high",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°": "low",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ": "low",
    "éƒµä¾¿ç•ªå·": "high",
    "éƒ½é“åºœçœŒ": "high",
    "ä½æ‰€": "high"
  }
}
"""

# å¿…é ˆé …ç›®ï¼ˆç©ºæ¬„ã ã¨ä¿¡é ¼åº¦ãŒä¸‹ãŒã‚‹é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
REQUIRED_FIELDS = [
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
]


# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---


MAX_IMAGE_BYTES = 4_500_000  # base64å¤‰æ›å¾Œã«5MBä»¥å†…ã«åã¾ã‚‹ã‚ˆã†ä½™è£•ã‚’æŒãŸã›ã‚‹


def compress_image(image_bytes: bytes, media_type: str) -> tuple[bytes, str]:
    """ç”»åƒãŒAPIã®ä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆã«ãƒªã‚µã‚¤ã‚ºãƒ»åœ§ç¸®ã™ã‚‹"""
    from PIL import Image

    if len(image_bytes) <= MAX_IMAGE_BYTES:
        return image_bytes, media_type

    img = Image.open(io.BytesIO(image_bytes))

    # JPEGåœ§ç¸®ã§ç¸®å°ã‚’è©¦ã¿ã‚‹ï¼ˆå“è³ªã‚’æ®µéšçš„ã«ä¸‹ã’ã‚‹ï¼‰
    for quality in (85, 70, 50, 35):
        # é•·è¾ºãŒå¤§ãã™ãã‚‹å ´åˆã¯ãƒªã‚µã‚¤ã‚º
        max_dim = 2048 if quality >= 70 else 1600
        if max(img.size) > max_dim:
            img.thumbnail((max_dim, max_dim), Image.LANCZOS)

        buf = io.BytesIO()
        rgb_img = img.convert("RGB") if img.mode != "RGB" else img
        rgb_img.save(buf, format="JPEG", quality=quality)
        result = buf.getvalue()
        if len(result) <= MAX_IMAGE_BYTES:
            return result, "image/jpeg"

    # æœ€çµ‚æ‰‹æ®µ: ã•ã‚‰ã«å°ã•ããƒªã‚µã‚¤ã‚º
    img.thumbnail((1200, 1200), Image.LANCZOS)
    buf = io.BytesIO()
    img.convert("RGB").save(buf, format="JPEG", quality=30)
    return buf.getvalue(), "image/jpeg"


def encode_image_to_base64(image_bytes: bytes) -> str:
    """ç”»åƒãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’base64æ–‡å­—åˆ—ã«å¤‰æ›"""
    return base64.b64encode(image_bytes).decode("utf-8")


def convert_pdf_to_image(pdf_bytes: bytes) -> Optional[bytes]:
    """PDFã®1ãƒšãƒ¼ã‚¸ç›®ã‚’ç”»åƒã«å¤‰æ›"""
    try:
        from pdf2image import convert_from_bytes

        images = convert_from_bytes(pdf_bytes, first_page=1, last_page=1, dpi=200)
        if images:
            buf = io.BytesIO()
            images[0].save(buf, format="PNG")
            return buf.getvalue()
    except ImportError:
        st.error(
            "pdf2imageãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            "ã¾ãŸã€Popplerã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚‚å¿…è¦ã§ã™ã€‚\n"
            "Windows: https://github.com/oschwartz10612/poppler-windows/releases ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
            "Mac: `brew install poppler`\n"
            "Linux: `sudo apt-get install poppler-utils`"
        )
    except Exception as e:
        st.error(f"PDFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    return None


def extract_with_anthropic(image_base64: str, media_type: str) -> Optional[dict]:
    """Anthropic Claude Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        import anthropic

        api_key = os.getenv("ANTHROPIC_API_KEY", "")
        if not api_key or api_key.startswith("sk-ant-xxx"):
            st.error("ANTHROPIC_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = anthropic.Anthropic(api_key=api_key)
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": media_type,
                                "data": image_base64,
                            },
                        },
                        {
                            "type": "text",
                            "text": EXTRACTION_PROMPT,
                        },
                    ],
                }
            ],
        )
        response_text = message.content[0].text
        return parse_json_response(response_text)
    except Exception as e:
        st.error(f"Anthropic API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def extract_with_openai(image_base64: str, media_type: str) -> Optional[dict]:
    """OpenAI GPT-4 Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        from openai import OpenAI

        api_key = os.getenv("OPENAI_API_KEY", "")
        if not api_key or api_key.startswith("sk-xxx"):
            st.error("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-4o",
            max_tokens=2048,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{media_type};base64,{image_base64}",
                            },
                        },
                        {
                            "type": "text",
                            "text": EXTRACTION_PROMPT,
                        },
                    ],
                }
            ],
        )
        response_text = response.choices[0].message.content
        return parse_json_response(response_text)
    except Exception as e:
        st.error(f"OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def parse_json_response(text: str) -> Optional[dict]:
    """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    # ```json ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã®æŠ½å‡ºã‚’è©¦ã¿ã‚‹
    if "```" in text:
        start = text.find("```")
        end = text.rfind("```")
        if start != end:
            json_block = text[start:end]
            # ```json ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
            first_newline = json_block.find("\n")
            if first_newline != -1:
                json_block = json_block[first_newline + 1 :]
            try:
                return json.loads(json_block.strip())
            except json.JSONDecodeError:
                pass
    # { ... } ã‚’ç›´æ¥æ¢ã™
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        try:
            return json.loads(text[start : end + 1])
        except json.JSONDecodeError:
            pass
    st.error("AIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’JSONå½¢å¼ã§è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.code(text, language="text")
    return None


def calc_confidence(data: dict) -> tuple[int, str, list[str]]:
    """
    ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ç…§åˆç‡(%)ãƒ»åˆ¤å®šãƒ©ãƒ™ãƒ«ãƒ»è¦ç¢ºèªé …ç›®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚

    ã‚¹ã‚³ã‚¢è¨ˆç®—:
    - å„é …ç›®ã«å€¤ãŒã‚ã‚Œã°åŠ ç‚¹ã€ç©ºæ¬„ãªã‚‰0ç‚¹
    - AIãŒ "high" ã¨å›ç­”ã—ãŸé …ç›®ã¯æº€ç‚¹ã€"low" ã¯åŠåˆ†
    - å¿…é ˆé …ç›®ã¯é…ç‚¹2å€
    """
    confidence_map = data.get("confidence", {})
    total_weight = 0
    earned = 0
    low_fields = []

    for col in CSV_COLUMNS:
        weight = 2 if col in REQUIRED_FIELDS else 1
        total_weight += weight
        val = str(data.get(col, "")).strip()
        ai_conf = confidence_map.get(col, "high" if val else "low")

        if not val:
            low_fields.append(col)
        elif ai_conf == "low":
            earned += weight * 0.5
            low_fields.append(col)
        else:
            earned += weight

    pct = int(earned / total_weight * 100) if total_weight else 0

    if pct >= 90 and not any(
        col in low_fields for col in REQUIRED_FIELDS
    ):
        label = "OK"
    elif pct >= 60:
        label = "è¦ç¢ºèª"
    else:
        label = "è¦ç¢ºèª(ä½)"

    return pct, label, low_fields


def build_dataframe(data_list: list[dict]) -> tuple[pd.DataFrame, list[dict]]:
    """æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã‹ã‚‰DataFrame+ä¿¡é ¼åº¦æƒ…å ±ã‚’æ§‹ç¯‰"""
    rows = []
    confidence_info = []
    for data in data_list:
        pct, label, low_fields = calc_confidence(data)
        row = {"åˆ¤å®š": label, "ç…§åˆç‡": f"{pct}%"}
        for col in CSV_COLUMNS:
            row[col] = data.get(col, "")
        rows.append(row)
        confidence_info.append({
            "pct": pct,
            "label": label,
            "low_fields": low_fields,
        })
    display_cols = ["åˆ¤å®š", "ç…§åˆç‡"] + CSV_COLUMNS
    return pd.DataFrame(rows, columns=display_cols), confidence_info


def _match_key(row: dict) -> Optional[str]:
    """çªåˆã‚­ãƒ¼ã‚’ç”Ÿæˆã€‚å—çµ¦è€…è¨¼ç•ªå·å„ªå…ˆã€ãªã‘ã‚Œã°å§“å+ç”Ÿå¹´æœˆæ—¥"""
    cert = str(row.get("éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·", "")).strip()
    if cert:
        return f"cert:{cert}"
    sei = str(row.get("åˆ©ç”¨è€…_å§“", "")).strip()
    mei = str(row.get("åˆ©ç”¨è€…_å", "")).strip()
    birth = str(row.get("ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)", "")).strip()
    if sei and mei and birth:
        return f"name:{sei}|{mei}|{birth}"
    return None


def merge_records(data_list: list[dict]) -> list[dict]:
    """åŒä¸€äººç‰©ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’çªåˆã—ã€ç©ºæ¬„ã‚’ã§ãã‚‹ã ã‘åŸ‹ã‚ãŸãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    from collections import OrderedDict

    groups: OrderedDict[str, dict] = OrderedDict()
    unmatched = []

    for data in data_list:
        key = _match_key(data)
        if key is None:
            unmatched.append(data)
            continue

        if key not in groups:
            merged = {col: data.get(col, "") for col in CSV_COLUMNS}
            # confidenceæƒ…å ±ã‚‚ã‚³ãƒ”ãƒ¼
            merged["confidence"] = dict(data.get("confidence", {}))
            groups[key] = merged
        else:
            existing = groups[key]
            existing_conf = existing.get("confidence", {})
            new_conf = data.get("confidence", {})
            for col in CSV_COLUMNS:
                new_val = str(data.get(col, "")).strip()
                old_val = str(existing.get(col, "")).strip()
                new_c = new_conf.get(col, "low")
                old_c = existing_conf.get(col, "low")
                if not old_val and new_val:
                    existing[col] = new_val
                    existing_conf[col] = new_c
                elif old_val and new_val:
                    # ä¸¡æ–¹ã‚ã‚‹å ´åˆ: highå„ªå…ˆã€åŒã˜ãªã‚‰ã‚ˆã‚Šé•·ã„å€¤ã‚’æ¡ç”¨
                    if new_c == "high" and old_c == "low":
                        existing[col] = new_val
                        existing_conf[col] = "high"
                    elif len(new_val) > len(old_val) and new_c == old_c:
                        existing[col] = new_val
            existing["confidence"] = existing_conf

    return list(groups.values()) + unmatched


def to_csv_bytes(df: pd.DataFrame) -> bytes:
    """DataFrameã‚’UTF-8 BOMä»˜ãCSVãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›"""
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


def append_to_google_sheet(df: pd.DataFrame, spreadsheet_url: str, sheet_name: str) -> int:
    """Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æœ«å°¾ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜ã—ã€è¿½è¨˜ã—ãŸè¡Œæ•°ã‚’è¿”ã™"""
    import gspread

    creds_path = Path(__file__).parent / "credentials.json"
    if not creds_path.exists():
        raise FileNotFoundError(
            "credentials.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            "Google Cloud ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ "
            f"{creds_path} ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚"
        )

    gc = gspread.service_account(filename=str(creds_path))
    sh = gc.open_by_url(spreadsheet_url)
    worksheet = sh.worksheet(sheet_name)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒç„¡ã„å ´åˆã¯1è¡Œç›®ã«è¿½åŠ 
    existing = worksheet.get_all_values()
    if not existing:
        worksheet.append_row(CSV_COLUMNS, value_input_option="USER_ENTERED")

    # DataFrameã®å„è¡Œã‚’è¿½è¨˜
    rows = df.fillna("").astype(str).values.tolist()
    worksheet.append_rows(rows, value_input_option="USER_ENTERED")
    return len(rows)


# --- Streamlit UI ---


def main():
    st.set_page_config(
        page_title="åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ„ãƒ¼ãƒ«",
        page_icon="ğŸ“‹",
        layout="wide",
    )

    st.title("åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«")
    st.caption("éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ â€” å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æŠ½å‡ºãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

    # APIé¸æŠï¼ˆå›ºå®š: Anthropicï¼‰
    api_provider = "Anthropic (Claude)"

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯¾å¿œï¼‰
    st.header("â‘  æ›¸é¡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=["jpg", "jpeg", "png", "pdf"],
        accept_multiple_files=True,
        help="å¯¾å¿œå½¢å¼: JPG, PNG, PDF (PDFã¯1ãƒšãƒ¼ã‚¸ç›®ã®ã¿å‡¦ç†) â€” è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠå¯",
    )

    if not uploaded_files:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨å‡¦ç†ãŒé–‹å§‹ã§ãã¾ã™ã€‚")
        return

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    images = []  # [(file_name, image_bytes, media_type), ...]
    for uf in uploaded_files:
        file_bytes = uf.read()
        file_name = uf.name
        is_pdf = file_name.lower().endswith(".pdf")

        if is_pdf:
            image_bytes = convert_pdf_to_image(file_bytes)
            if image_bytes is None:
                st.error(f"PDFã®ç”»åƒå¤‰æ›ã«å¤±æ•—: {file_name}")
                continue
            media_type = "image/png"
        else:
            image_bytes = file_bytes
            ext = file_name.lower().rsplit(".", 1)[-1]
            media_type = "image/jpeg" if ext in ("jpg", "jpeg") else "image/png"
        images.append((file_name, image_bytes, media_type))

    if not images:
        return

    # ã‚¹ãƒ†ãƒƒãƒ—2: ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    st.header(f"â‘¡ ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆ{len(images)}ä»¶ï¼‰")
    cols = st.columns(min(len(images), 3))
    for i, (fname, img_bytes, _) in enumerate(images):
        with cols[i % 3]:
            st.image(img_bytes, caption=fname, use_container_width=True)

    # ã‚¹ãƒ†ãƒƒãƒ—3: AIæŠ½å‡ºï¼ˆä¸€æ‹¬ï¼‰
    st.header("â‘¢ AIã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")

    if st.button("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹", type="primary", use_container_width=True):
        results = []
        progress = st.progress(0, text="æŠ½å‡ºä¸­...")
        for i, (fname, img_bytes, mtype) in enumerate(images):
            progress.progress(
                i / len(images),
                text=f"æŠ½å‡ºä¸­... ({i + 1}/{len(images)}) {fname}",
            )
            compressed, comp_mtype = compress_image(img_bytes, mtype)
            image_base64 = encode_image_to_base64(compressed)
            if api_provider == "Anthropic (Claude)":
                extracted = extract_with_anthropic(image_base64, comp_mtype)
            else:
                extracted = extract_with_openai(image_base64, comp_mtype)

            if extracted is not None:
                results.append(extracted)
            else:
                st.warning(f"æŠ½å‡ºå¤±æ•—: {fname}")
                # ç©ºè¡Œã‚’è¿½åŠ ã—ã¦ä»¶æ•°ã‚’ç¶­æŒ
                results.append({col: "" for col in CSV_COLUMNS})

        progress.progress(1.0, text=f"å®Œäº†ï¼ {len(results)}ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        # åŒä¸€äººç‰©ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’çªåˆ
        merged = merge_records(results)
        st.session_state["extracted_data"] = merged
        st.session_state["raw_count"] = len(results)
        st.rerun()

    # ã‚¹ãƒ†ãƒƒãƒ—4: çµæœç¢ºèªãƒ»ç·¨é›†
    if "extracted_data" in st.session_state:
        data_list = st.session_state["extracted_data"]
        raw_count = st.session_state.get("raw_count", len(data_list))

        df, conf_info = build_dataframe(data_list)
        ok_count = sum(1 for c in conf_info if c["label"] == "OK")
        review_count = len(conf_info) - ok_count

        if raw_count != len(data_list):
            st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{raw_count}ä»¶ â†’ çªåˆå¾Œ {len(data_list)}ä»¶ï¼‰")
        else:
            st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{len(data_list)}ä»¶ï¼‰")

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        col1.metric("OK (ç¢ºèªä¸è¦)", f"{ok_count}ä»¶")
        col2.metric("è¦ç¢ºèª", f"{review_count}ä»¶")
        col3.metric("å¹³å‡ç…§åˆç‡", f"{sum(c['pct'] for c in conf_info) // len(conf_info)}%")

        # ãƒ•ã‚£ãƒ«ã‚¿
        view_filter = st.radio(
            "è¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿",
            ["ã™ã¹ã¦", "è¦ç¢ºèªã®ã¿", "OKã®ã¿"],
            horizontal=True,
        )

        if view_filter == "è¦ç¢ºèªã®ã¿":
            mask = df["åˆ¤å®š"] != "OK"
            display_df = df[mask].reset_index(drop=True)
            display_conf = [c for c in conf_info if c["label"] != "OK"]
        elif view_filter == "OKã®ã¿":
            mask = df["åˆ¤å®š"] == "OK"
            display_df = df[mask].reset_index(drop=True)
            display_conf = [c for c in conf_info if c["label"] == "OK"]
        else:
            display_df = df
            display_conf = conf_info

        # è¦ç¢ºèªé …ç›®ã®è©³ç´°è¡¨ç¤º
        if display_conf and any(c["low_fields"] for c in display_conf):
            with st.expander("è¦ç¢ºèªé …ç›®ã®è©³ç´°", expanded=review_count > 0):
                for i, c in enumerate(display_conf):
                    if c["low_fields"]:
                        name = display_df.iloc[i].get("åˆ©ç”¨è€…_å§“", "") + " " + display_df.iloc[i].get("åˆ©ç”¨è€…_å", "")
                        name = name.strip() or f"è¡Œ{i+1}"
                        st.markdown(
                            f"**{name}** (ç…§åˆç‡ {c['pct']}%) â€” "
                            f"ä¸æ˜é …ç›®: {', '.join(c['low_fields'])}"
                        )

        st.caption("å„ã‚»ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç›´æ¥ä¿®æ­£ã§ãã¾ã™ã€‚ã€Œåˆ¤å®šã€ã€Œç…§åˆç‡ã€åˆ—ã¯å‡ºåŠ›ã«å«ã¾ã‚Œã¾ã›ã‚“ã€‚")
        edited_df = st.data_editor(
            display_df,
            use_container_width=True,
            num_rows="dynamic",
            disabled=["åˆ¤å®š", "ç…§åˆç‡"],
            key="data_editor",
        )

        # CSVå‡ºåŠ›ç”¨ã«ã¯ãƒ‡ãƒ¼ã‚¿åˆ—ã®ã¿æŠ½å‡º
        export_df = edited_df[CSV_COLUMNS] if all(c in edited_df.columns for c in CSV_COLUMNS) else edited_df

        # ã‚¹ãƒ†ãƒƒãƒ—5: CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.header("â‘¤ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        csv_bytes = to_csv_bytes(export_df)
        st.download_button(
            label=f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ{len(export_df)}ä»¶ï¼‰",
            data=csv_bytes,
            file_name="åˆ©ç”¨è€…æƒ…å ±.csv",
            mime="text/csv",
            use_container_width=True,
        )

        # ã‚¹ãƒ†ãƒƒãƒ—5': ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸åæ˜ ï¼ˆPoCï¼‰
        st.header("â‘¤' ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸åæ˜ ")

        spreadsheet_url = st.text_input(
            "Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL",
            value=st.session_state.get("spreadsheet_url", ""),
            placeholder="https://docs.google.com/spreadsheets/d/xxxxx/edit",
        )
        sheet_name = st.text_input(
            "ã‚·ãƒ¼ãƒˆå",
            value=st.session_state.get("sheet_name", "ã‚·ãƒ¼ãƒˆ1"),
        )
        st.session_state["spreadsheet_url"] = spreadsheet_url
        st.session_state["sheet_name"] = sheet_name

        if st.button(
            f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã‚‹ï¼ˆ{len(export_df)}ä»¶ï¼‰",
            type="primary",
            use_container_width=True,
            disabled=not spreadsheet_url,
        ):
            try:
                with st.spinner("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ä¸­..."):
                    count = append_to_google_sheet(export_df, spreadsheet_url, sheet_name)
                st.success(f"{count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
            except FileNotFoundError as e:
                st.error(str(e))
            except Exception as e:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()
