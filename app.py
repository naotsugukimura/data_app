"""
åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼‰
éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ - å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æŠ½å‡º
"""

import base64
import io
import json
import os
import re
from pathlib import Path
from typing import Optional

import pandas as pd
import streamlit as st
from dotenv import load_dotenv

# app.pyã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®.envã‚’æ˜ç¤ºçš„ã«èª­ã¿è¾¼ã‚€ï¼ˆã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒå¤‰æ•°ã‚ˆã‚Šå„ªå…ˆï¼‰
load_dotenv(Path(__file__).parent / ".env", override=True)


def get_secret(key: str) -> str:
    """Streamlit Cloud ã® secrets â†’ .env ã®é †ã§ã‚­ãƒ¼ã‚’å–å¾—"""
    try:
        return st.secrets[key]
    except (KeyError, FileNotFoundError):
        return os.getenv(key, "")

# --- å®šæ•°å®šç¾© ---

CSV_COLUMNS = [
    "ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å",
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)",
    "åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)",
    "æ€§åˆ¥",
    "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ",
    "éƒµä¾¿ç•ªå·",
    "éƒ½é“åºœçœŒ",
    "ä½æ‰€",
]

EXTRACTION_PROMPT = """ã‚ãªãŸã¯éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹ã®æ›¸é¡èª­ã¿å–ã‚Šå°‚é–€ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã¯ã€Œå—çµ¦è€…è¨¼ã€ã¾ãŸã¯ã€Œå¥‘ç´„æ›¸ã€ã§ã™ã€‚

ä»¥ä¸‹ã®é …ç›®ã‚’ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Šã€JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
èª­ã¿å–ã‚Œãªã„é …ç›®ã¯ç©ºæ–‡å­—("")ã¨ã—ã¦ãã ã•ã„ã€‚

å„é …ç›®ã«ã¤ã„ã¦ã€èª­ã¿å–ã‚Šã®ç¢ºä¿¡åº¦ã‚’ "confidence" ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
- "high": ã¯ã£ãã‚Šèª­ã¿å–ã‚ŒãŸ
- "low": æ–‡å­—ãŒä¸é®®æ˜ã€æ¨æ¸¬ãŒå«ã¾ã‚Œã‚‹ã€ã¾ãŸã¯è©²å½“é …ç›®ãŒæ›¸é¡ä¸Šã«è¦‹å½“ãŸã‚‰ãªã„ãŒæ¨å®šã—ãŸ

æŠ½å‡ºé …ç›®:
1. ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å
2. åˆ©ç”¨è€…_å§“
3. åˆ©ç”¨è€…_å
4. åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)
5. åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)
6. æ€§åˆ¥ (ç”· or å¥³)
7. ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥) ä¾‹: 1990å¹´01æœˆ15æ—¥
8. éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·
9. æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥) ä¾‹: 2024å¹´04æœˆ01æ—¥
10. æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥) ä¾‹: 2025å¹´03æœˆ31æ—¥
11. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°
12. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ
13. éƒµä¾¿ç•ªå· (ãƒã‚¤ãƒ•ãƒ³ãªã—7æ¡ã®æ•°å­—ã®ã¿ ä¾‹: 1234567)
14. éƒ½é“åºœçœŒ
15. ä½æ‰€ (éƒ½é“åºœçœŒã‚ˆã‚Šå¾Œã®éƒ¨åˆ†)

å›ç­”ã¯JSONå½¢å¼ã®ã¿ã§ã€ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚ä»¥ä¸‹ã®å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„:
{
  "ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å": "",
  "åˆ©ç”¨è€…_å§“": "",
  "åˆ©ç”¨è€…_å": "",
  "åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)": "",
  "åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)": "",
  "æ€§åˆ¥": "",
  "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "",
  "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·": "",
  "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "",
  "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°": "",
  "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ": "",
  "éƒµä¾¿ç•ªå·": "",
  "éƒ½é“åºœçœŒ": "",
  "ä½æ‰€": "",
  "confidence": {
    "ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨æ‹ ç‚¹å": "high",
    "åˆ©ç”¨è€…_å§“": "high",
    "åˆ©ç”¨è€…_å": "high",
    "åˆ©ç”¨è€…_å§“(ãµã‚ŠãŒãª)": "low",
    "åˆ©ç”¨è€…_å(ãµã‚ŠãŒãª)": "low",
    "æ€§åˆ¥": "high",
    "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "high",
    "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·": "high",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "high",
    "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)": "high",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_æœˆæ•°": "low",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°_çµ‚äº†æœˆ": "low",
    "éƒµä¾¿ç•ªå·": "high",
    "éƒ½é“åºœçœŒ": "high",
    "ä½æ‰€": "high"
  }
}
"""

# å¿…é ˆé …ç›®ï¼ˆç©ºæ¬„ã ã¨ä¿¡é ¼åº¦ãŒä¸‹ãŒã‚‹é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
REQUIRED_FIELDS = [
    "åˆ©ç”¨è€…_å§“",
    "åˆ©ç”¨è€…_å",
    "ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·",
    "æ”¯çµ¦æ±ºå®šé–‹å§‹æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
    "æ”¯çµ¦æ±ºå®šçµ‚äº†æ—¥ (YYYYå¹´MMæœˆDDæ—¥)",
]

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸Šé™æšæ•°
MAX_FILES = 100
# ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«è¡¨ç¤ºã™ã‚‹æœ€å¤§æšæ•°
PREVIEW_MAX = 12
# ãƒãƒƒãƒå‡¦ç†ã®å˜ä½
BATCH_SIZE = 10


# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---


MAX_IMAGE_BYTES = 4_500_000  # base64å¤‰æ›å¾Œã«5MBä»¥å†…ã«åã¾ã‚‹ã‚ˆã†ä½™è£•ã‚’æŒãŸã›ã‚‹


def compress_image(image_bytes: bytes, media_type: str) -> tuple[bytes, str]:
    """ç”»åƒãŒAPIã®ä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆã«ãƒªã‚µã‚¤ã‚ºãƒ»åœ§ç¸®ã™ã‚‹"""
    from PIL import Image

    if len(image_bytes) <= MAX_IMAGE_BYTES:
        return image_bytes, media_type

    img = Image.open(io.BytesIO(image_bytes))

    # JPEGåœ§ç¸®ã§ç¸®å°ã‚’è©¦ã¿ã‚‹ï¼ˆå“è³ªã‚’æ®µéšçš„ã«ä¸‹ã’ã‚‹ï¼‰
    for quality in (85, 70, 50, 35):
        # é•·è¾ºãŒå¤§ãã™ãã‚‹å ´åˆã¯ãƒªã‚µã‚¤ã‚º
        max_dim = 2048 if quality >= 70 else 1600
        if max(img.size) > max_dim:
            img.thumbnail((max_dim, max_dim), Image.LANCZOS)

        buf = io.BytesIO()
        rgb_img = img.convert("RGB") if img.mode != "RGB" else img
        rgb_img.save(buf, format="JPEG", quality=quality)
        result = buf.getvalue()
        if len(result) <= MAX_IMAGE_BYTES:
            return result, "image/jpeg"

    # æœ€çµ‚æ‰‹æ®µ: ã•ã‚‰ã«å°ã•ããƒªã‚µã‚¤ã‚º
    img.thumbnail((1200, 1200), Image.LANCZOS)
    buf = io.BytesIO()
    img.convert("RGB").save(buf, format="JPEG", quality=30)
    return buf.getvalue(), "image/jpeg"


def encode_image_to_base64(image_bytes: bytes) -> str:
    """ç”»åƒãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’base64æ–‡å­—åˆ—ã«å¤‰æ›"""
    return base64.b64encode(image_bytes).decode("utf-8")


def convert_pdf_to_image(pdf_bytes: bytes) -> Optional[bytes]:
    """PDFã®1ãƒšãƒ¼ã‚¸ç›®ã‚’ç”»åƒã«å¤‰æ›"""
    try:
        from pdf2image import convert_from_bytes

        images = convert_from_bytes(pdf_bytes, first_page=1, last_page=1, dpi=200)
        if images:
            buf = io.BytesIO()
            images[0].save(buf, format="PNG")
            return buf.getvalue()
    except ImportError:
        st.error(
            "pdf2imageãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
            "ã¾ãŸã€Popplerã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚‚å¿…è¦ã§ã™ã€‚\n"
            "Windows: https://github.com/oschwartz10612/poppler-windows/releases ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
            "Mac: `brew install poppler`\n"
            "Linux: `sudo apt-get install poppler-utils`"
        )
    except Exception as e:
        st.error(f"PDFå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    return None


def strip_postal_hyphen(val: str) -> str:
    """éƒµä¾¿ç•ªå·ã‹ã‚‰ãƒã‚¤ãƒ•ãƒ³ã‚’é™¤å»ã—ã¦æ•°å­—7æ¡ã®ã¿ã«ã™ã‚‹"""
    digits = re.sub(r"[^\d]", "", val)
    return digits


def extract_with_anthropic(image_base64: str, media_type: str) -> Optional[dict]:
    """Anthropic Claude Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        import anthropic

        api_key = get_secret("ANTHROPIC_API_KEY")
        if not api_key or api_key.startswith("sk-ant-xxx"):
            st.error("ANTHROPIC_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = anthropic.Anthropic(api_key=api_key)
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": media_type,
                                "data": image_base64,
                            },
                        },
                        {
                            "type": "text",
                            "text": EXTRACTION_PROMPT,
                        },
                    ],
                }
            ],
        )
        response_text = message.content[0].text
        result = parse_json_response(response_text)
        if result:
            # éƒµä¾¿ç•ªå·ã®ãƒã‚¤ãƒ•ãƒ³é™¤å»ã‚’å¾Œå‡¦ç†ã§ã‚‚ä¿è¨¼
            postal = str(result.get("éƒµä¾¿ç•ªå·", ""))
            if postal:
                result["éƒµä¾¿ç•ªå·"] = strip_postal_hyphen(postal)
        return result
    except Exception as e:
        st.error(f"Anthropic API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def extract_with_openai(image_base64: str, media_type: str) -> Optional[dict]:
    """OpenAI GPT-4 Vision APIã§ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        from openai import OpenAI

        api_key = get_secret("OPENAI_API_KEY")
        if not api_key or api_key.startswith("sk-xxx"):
            st.error("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None

        client = OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-4o",
            max_tokens=2048,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{media_type};base64,{image_base64}",
                            },
                        },
                        {
                            "type": "text",
                            "text": EXTRACTION_PROMPT,
                        },
                    ],
                }
            ],
        )
        response_text = response.choices[0].message.content
        result = parse_json_response(response_text)
        if result:
            postal = str(result.get("éƒµä¾¿ç•ªå·", ""))
            if postal:
                result["éƒµä¾¿ç•ªå·"] = strip_postal_hyphen(postal)
        return result
    except Exception as e:
        st.error(f"OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def parse_json_response(text: str) -> Optional[dict]:
    """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    # ```json ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã®æŠ½å‡ºã‚’è©¦ã¿ã‚‹
    if "```" in text:
        start = text.find("```")
        end = text.rfind("```")
        if start != end:
            json_block = text[start:end]
            # ```json ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
            first_newline = json_block.find("\n")
            if first_newline != -1:
                json_block = json_block[first_newline + 1 :]
            try:
                return json.loads(json_block.strip())
            except json.JSONDecodeError:
                pass
    # { ... } ã‚’ç›´æ¥æ¢ã™
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        try:
            return json.loads(text[start : end + 1])
        except json.JSONDecodeError:
            pass
    st.error("AIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’JSONå½¢å¼ã§è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.code(text, language="text")
    return None


def calc_confidence(data: dict) -> tuple[int, str, list[str]]:
    """
    ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ç…§åˆç‡(%)ãƒ»åˆ¤å®šãƒ©ãƒ™ãƒ«ãƒ»è¦ç¢ºèªé …ç›®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚

    ã‚¹ã‚³ã‚¢è¨ˆç®—:
    - å„é …ç›®ã«å€¤ãŒã‚ã‚Œã°åŠ ç‚¹ã€ç©ºæ¬„ãªã‚‰0ç‚¹
    - AIãŒ "high" ã¨å›ç­”ã—ãŸé …ç›®ã¯æº€ç‚¹ã€"low" ã¯åŠåˆ†
    - å¿…é ˆé …ç›®ã¯é…ç‚¹2å€
    """
    confidence_map = data.get("confidence", {})
    total_weight = 0
    earned = 0
    low_fields = []

    for col in CSV_COLUMNS:
        weight = 2 if col in REQUIRED_FIELDS else 1
        total_weight += weight
        val = str(data.get(col, "")).strip()
        ai_conf = confidence_map.get(col, "high" if val else "low")

        if not val:
            low_fields.append(col)
        elif ai_conf == "low":
            earned += weight * 0.5
            low_fields.append(col)
        else:
            earned += weight

    pct = int(earned / total_weight * 100) if total_weight else 0

    if pct >= 90 and not any(
        col in low_fields for col in REQUIRED_FIELDS
    ):
        label = "OK"
    elif pct >= 60:
        label = "è¦ç¢ºèª"
    else:
        label = "è¦ç¢ºèª(ä½)"

    return pct, label, low_fields


def build_dataframe(data_list: list[dict]) -> tuple[pd.DataFrame, list[dict]]:
    """æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã‹ã‚‰DataFrame+ä¿¡é ¼åº¦æƒ…å ±ã‚’æ§‹ç¯‰"""
    rows = []
    confidence_info = []
    for data in data_list:
        pct, label, low_fields = calc_confidence(data)
        row = {"åˆ¤å®š": label, "ç…§åˆç‡": f"{pct}%"}
        for col in CSV_COLUMNS:
            row[col] = data.get(col, "")
        rows.append(row)
        confidence_info.append({
            "pct": pct,
            "label": label,
            "low_fields": low_fields,
        })
    display_cols = ["åˆ¤å®š", "ç…§åˆç‡"] + CSV_COLUMNS
    return pd.DataFrame(rows, columns=display_cols), confidence_info


def _match_key(row: dict) -> Optional[str]:
    """çªåˆã‚­ãƒ¼ã‚’ç”Ÿæˆã€‚å—çµ¦è€…è¨¼ç•ªå·å„ªå…ˆã€ãªã‘ã‚Œã°å§“å+ç”Ÿå¹´æœˆæ—¥"""
    cert = str(row.get("éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹å—çµ¦è€…è¨¼ç•ªå·", "")).strip()
    if cert:
        return f"cert:{cert}"
    sei = str(row.get("åˆ©ç”¨è€…_å§“", "")).strip()
    mei = str(row.get("åˆ©ç”¨è€…_å", "")).strip()
    birth = str(row.get("ç”Ÿå¹´æœˆæ—¥ (YYYYå¹´MMæœˆDDæ—¥)", "")).strip()
    if sei and mei and birth:
        return f"name:{sei}|{mei}|{birth}"
    return None


def merge_records(data_list: list[dict]) -> list[dict]:
    """åŒä¸€äººç‰©ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’çªåˆã—ã€ç©ºæ¬„ã‚’ã§ãã‚‹ã ã‘åŸ‹ã‚ãŸãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    from collections import OrderedDict

    groups: OrderedDict[str, dict] = OrderedDict()
    unmatched = []

    for data in data_list:
        key = _match_key(data)
        if key is None:
            unmatched.append(data)
            continue

        if key not in groups:
            merged = {col: data.get(col, "") for col in CSV_COLUMNS}
            # confidenceæƒ…å ±ã‚‚ã‚³ãƒ”ãƒ¼
            merged["confidence"] = dict(data.get("confidence", {}))
            groups[key] = merged
        else:
            existing = groups[key]
            existing_conf = existing.get("confidence", {})
            new_conf = data.get("confidence", {})
            for col in CSV_COLUMNS:
                new_val = str(data.get(col, "")).strip()
                old_val = str(existing.get(col, "")).strip()
                new_c = new_conf.get(col, "low")
                old_c = existing_conf.get(col, "low")
                if not old_val and new_val:
                    existing[col] = new_val
                    existing_conf[col] = new_c
                elif old_val and new_val:
                    # ä¸¡æ–¹ã‚ã‚‹å ´åˆ: highå„ªå…ˆã€åŒã˜ãªã‚‰ã‚ˆã‚Šé•·ã„å€¤ã‚’æ¡ç”¨
                    if new_c == "high" and old_c == "low":
                        existing[col] = new_val
                        existing_conf[col] = "high"
                    elif len(new_val) > len(old_val) and new_c == old_c:
                        existing[col] = new_val
            existing["confidence"] = existing_conf

    return list(groups.values()) + unmatched


def to_csv_bytes(df: pd.DataFrame) -> bytes:
    """DataFrameã‚’UTF-8 BOMä»˜ãCSVãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›"""
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


def append_to_google_sheet(df: pd.DataFrame, spreadsheet_url: str, sheet_name: str) -> int:
    """Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æœ«å°¾ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜ã—ã€è¿½è¨˜ã—ãŸè¡Œæ•°ã‚’è¿”ã™"""
    import gspread

    creds_path = Path(__file__).parent / "credentials.json"
    if not creds_path.exists():
        raise FileNotFoundError(
            "credentials.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            "Google Cloud ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ "
            f"{creds_path} ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚"
        )

    gc = gspread.service_account(filename=str(creds_path))
    sh = gc.open_by_url(spreadsheet_url)
    worksheet = sh.worksheet(sheet_name)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒç„¡ã„å ´åˆã¯1è¡Œç›®ã«è¿½åŠ 
    existing = worksheet.get_all_values()
    if not existing:
        worksheet.append_row(CSV_COLUMNS, value_input_option="USER_ENTERED")

    # DataFrameã®å„è¡Œã‚’è¿½è¨˜
    rows = df.fillna("").astype(str).values.tolist()
    worksheet.append_rows(rows, value_input_option="USER_ENTERED")
    return len(rows)


# --- Streamlit UI ---


def inject_beforeunload_guard():
    """å‡¦ç†ä¸­ã«ã‚¿ãƒ–ã‚’é–‰ã˜ã‚ˆã†ã¨ã—ãŸã‚‰ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‡ºã™JS"""
    st.components.v1.html(
        """
        <script>
        // Streamlitã®è¦ªwindowã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¨­å®š
        const win = window.parent || window;
        win.addEventListener('beforeunload', function(e) {
            e.preventDefault();
            e.returnValue = '';
        });
        </script>
        """,
        height=0,
    )


def remove_beforeunload_guard():
    """å‡¦ç†å®Œäº†å¾Œã«ã‚¢ãƒ©ãƒ¼ãƒˆã‚’è§£é™¤ã™ã‚‹JS"""
    st.components.v1.html(
        """
        <script>
        const win = window.parent || window;
        win.onbeforeunload = null;
        </script>
        """,
        height=0,
    )


def main():
    st.set_page_config(
        page_title="åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ„ãƒ¼ãƒ«",
        page_icon="ğŸ“‹",
        layout="wide",
    )

    st.title("åˆ©ç”¨è€…æƒ…å ± è‡ªå‹•æŠ½å‡ºãƒ»CSVå‡ºåŠ›ãƒ„ãƒ¼ãƒ«")
    st.caption("éšœå®³ç¦ç¥‰ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­æ‰€å‘ã‘ â€” å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æŠ½å‡ºãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

    # APIé¸æŠï¼ˆå›ºå®š: Anthropicï¼‰
    api_provider = "Anthropic (Claude)"

    # å‡¦ç†ä¸­ãƒ•ãƒ©ã‚°ãŒã‚ã‚Œã°ã‚¿ãƒ–é–‰ã˜ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æœ‰åŠ¹åŒ–
    if st.session_state.get("processing"):
        inject_beforeunload_guard()

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯¾å¿œãƒ»ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—å¯¾å¿œï¼‰
    st.header("â‘  æ›¸é¡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    st.caption(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã€ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠï¼ˆæœ€å¤§{MAX_FILES}æšã¾ã§ï¼‰")
    uploaded_files = st.file_uploader(
        "å—çµ¦è€…è¨¼ãƒ»å¥‘ç´„æ›¸ã®ç”»åƒã¾ãŸã¯PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["jpg", "jpeg", "png", "pdf"],
        accept_multiple_files=True,
        label_visibility="collapsed",
    )

    if not uploaded_files:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã€ã¾ãŸã¯ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    if len(uploaded_files) > MAX_FILES:
        st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯{MAX_FILES}æšã¾ã§ã§ã™ã€‚ç¾åœ¨{len(uploaded_files)}æšé¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚")
        return

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç›´å¾Œã¯ãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§ã®ã¿è¡¨ç¤ºï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿ã¯èª­ã¿è¾¼ã¾ãªã„ï¼‰
    file_names = [uf.name for uf in uploaded_files]
    st.success(f"{len(file_names)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯æŠ˜ã‚ŠãŸãŸã¿å†…ã§é…å»¶è¡¨ç¤ºï¼‰
    st.header(f"â‘¡ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆ{len(file_names)}ä»¶ï¼‰")
    with st.expander("ãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§", expanded=False):
        for i, name in enumerate(file_names):
            st.text(f"{i+1}. {name}")

    with st.expander(f"ç”»åƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­{PREVIEW_MAX}ä»¶ï¼‰", expanded=False):
        preview_count = min(len(uploaded_files), PREVIEW_MAX)
        cols = st.columns(3)
        for i in range(preview_count):
            uf = uploaded_files[i]
            with cols[i % 3]:
                st.image(uf, caption=uf.name, use_container_width=True)

    # ã‚¹ãƒ†ãƒƒãƒ—3: AIæŠ½å‡ºï¼ˆãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã«ç”»åƒã‚’èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†ï¼‰
    st.header("â‘¢ AIã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")

    if st.button("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹", type="primary", use_container_width=True):
        st.session_state["processing"] = True
        inject_beforeunload_guard()

        results = []
        file_conf_map = {}  # file_name -> confidence_pct
        images_for_review = []  # ä¿¡é ¼å€¤ä½ã„å†™çœŸç”¨ã«ä¿æŒ
        progress = st.progress(0, text="æº–å‚™ä¸­...")
        status_text = st.empty()

        total = len(uploaded_files)
        for i, uf in enumerate(uploaded_files):
            fname = uf.name
            batch_num = i // BATCH_SIZE + 1
            total_batches = (total - 1) // BATCH_SIZE + 1
            progress.progress(
                i / total,
                text=f"æŠ½å‡ºä¸­... ({i + 1}/{total}) ãƒãƒƒãƒ {batch_num}/{total_batches}",
            )
            status_text.caption(f"å‡¦ç†ä¸­: {fname}")

            # ã“ã“ã§åˆã‚ã¦ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆé…å»¶èª­ã¿è¾¼ã¿ï¼‰
            file_bytes = uf.read()
            is_pdf = fname.lower().endswith(".pdf")

            if is_pdf:
                image_bytes = convert_pdf_to_image(file_bytes)
                if image_bytes is None:
                    st.warning(f"PDFå¤‰æ›å¤±æ•—: {fname}")
                    file_conf_map[fname] = 0
                    results.append({col: "" for col in CSV_COLUMNS})
                    continue
                mtype = "image/png"
            else:
                image_bytes = file_bytes
                ext = fname.lower().rsplit(".", 1)[-1]
                mtype = "image/jpeg" if ext in ("jpg", "jpeg") else "image/png"

            compressed, comp_mtype = compress_image(image_bytes, mtype)
            image_base64 = encode_image_to_base64(compressed)
            if api_provider == "Anthropic (Claude)":
                extracted = extract_with_anthropic(image_base64, comp_mtype)
            else:
                extracted = extract_with_openai(image_base64, comp_mtype)

            if extracted is not None:
                extracted["_source_file"] = fname
                results.append(extracted)
                pct, _, _ = calc_confidence(extracted)
                file_conf_map[fname] = pct
            else:
                st.warning(f"æŠ½å‡ºå¤±æ•—: {fname}")
                empty = {col: "" for col in CSV_COLUMNS}
                empty["_source_file"] = fname
                results.append(empty)
                file_conf_map[fname] = 0

            # ä¿¡é ¼å€¤ãŒä½ã„å†™çœŸã¯å¾Œã§è¡¨ç¤ºã™ã‚‹ãŸã‚ã«ä¿æŒ
            if file_conf_map[fname] < 90:
                images_for_review.append((fname, image_bytes))

        progress.progress(1.0, text=f"å®Œäº†ï¼ {len(results)}ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
        status_text.empty()

        # åŒä¸€äººç‰©ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’çªåˆ
        merged = merge_records(results)
        st.session_state["extracted_data"] = merged
        st.session_state["raw_count"] = len(results)
        st.session_state["file_conf_map"] = file_conf_map
        st.session_state["images_for_review"] = images_for_review
        st.session_state["processing"] = False
        st.rerun()

    # ã‚¹ãƒ†ãƒƒãƒ—3.5: ä¿¡é ¼å€¤ãŒä½ã„å†™çœŸã®ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
    if "images_for_review" in st.session_state and "file_conf_map" in st.session_state:
        file_conf_map = st.session_state["file_conf_map"]
        low_conf_images = st.session_state["images_for_review"]
        if low_conf_images:
            st.header(f"âš  èª­å–ç²¾åº¦ãŒä½ã„æ›¸é¡ï¼ˆ{len(low_conf_images)}ä»¶ï¼‰")
            st.caption("ä»¥ä¸‹ã®æ›¸é¡ã¯èª­ã¿å–ã‚Šä¿¡é ¼åº¦ãŒä½ã„ãŸã‚ã€æŠ½å‡ºçµæœã‚’é‡ç‚¹çš„ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            cols = st.columns(min(len(low_conf_images), 3))
            for i, (fname, img_bytes) in enumerate(low_conf_images):
                pct = file_conf_map.get(fname, 0)
                with cols[i % 3]:
                    st.image(img_bytes, use_container_width=True)
                    if pct < 60:
                        st.error(f"ğŸ“„ {fname}  â€”  ç…§åˆç‡ **{pct}%**")
                    else:
                        st.warning(f"ğŸ“„ {fname}  â€”  ç…§åˆç‡ **{pct}%**")

    # ã‚¹ãƒ†ãƒƒãƒ—4: çµæœç¢ºèªãƒ»ç·¨é›†
    if "extracted_data" in st.session_state:
        data_list = st.session_state["extracted_data"]
        raw_count = st.session_state.get("raw_count", len(data_list))

        df, conf_info = build_dataframe(data_list)
        ok_count = sum(1 for c in conf_info if c["label"] == "OK")
        review_count = len(conf_info) - ok_count

        if raw_count != len(data_list):
            st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{raw_count}ä»¶ â†’ çªåˆå¾Œ {len(data_list)}ä»¶ï¼‰")
        else:
            st.header(f"â‘£ æŠ½å‡ºçµæœã®ç¢ºèªãƒ»ç·¨é›†ï¼ˆ{len(data_list)}ä»¶ï¼‰")

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        col1.metric("OK (ç¢ºèªä¸è¦)", f"{ok_count}ä»¶")
        col2.metric("è¦ç¢ºèª", f"{review_count}ä»¶")
        col3.metric("å¹³å‡ç…§åˆç‡", f"{sum(c['pct'] for c in conf_info) // len(conf_info)}%")

        # ãƒ•ã‚£ãƒ«ã‚¿
        view_filter = st.radio(
            "è¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿",
            ["ã™ã¹ã¦", "è¦ç¢ºèªã®ã¿", "OKã®ã¿"],
            horizontal=True,
        )

        if view_filter == "è¦ç¢ºèªã®ã¿":
            mask = df["åˆ¤å®š"] != "OK"
            display_df = df[mask].reset_index(drop=True)
            display_conf = [c for c in conf_info if c["label"] != "OK"]
        elif view_filter == "OKã®ã¿":
            mask = df["åˆ¤å®š"] == "OK"
            display_df = df[mask].reset_index(drop=True)
            display_conf = [c for c in conf_info if c["label"] == "OK"]
        else:
            display_df = df
            display_conf = conf_info

        # è¦ç¢ºèªé …ç›®ã®è©³ç´°è¡¨ç¤º
        if display_conf and any(c["low_fields"] for c in display_conf):
            with st.expander("è¦ç¢ºèªé …ç›®ã®è©³ç´°", expanded=review_count > 0):
                for i, c in enumerate(display_conf):
                    if c["low_fields"]:
                        name = display_df.iloc[i].get("åˆ©ç”¨è€…_å§“", "") + " " + display_df.iloc[i].get("åˆ©ç”¨è€…_å", "")
                        name = name.strip() or f"è¡Œ{i+1}"
                        st.markdown(
                            f"**{name}** (ç…§åˆç‡ {c['pct']}%) â€” "
                            f"ä¸æ˜é …ç›®: {', '.join(c['low_fields'])}"
                        )

        st.caption("å„ã‚»ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç›´æ¥ä¿®æ­£ã§ãã¾ã™ã€‚ã€Œåˆ¤å®šã€ã€Œç…§åˆç‡ã€åˆ—ã¯å‡ºåŠ›ã«å«ã¾ã‚Œã¾ã›ã‚“ã€‚")
        edited_df = st.data_editor(
            display_df,
            use_container_width=True,
            num_rows="dynamic",
            disabled=["åˆ¤å®š", "ç…§åˆç‡"],
            key="data_editor",
        )

        # CSVå‡ºåŠ›ç”¨ã«ã¯ãƒ‡ãƒ¼ã‚¿åˆ—ã®ã¿æŠ½å‡º
        export_df = edited_df[CSV_COLUMNS] if all(c in edited_df.columns for c in CSV_COLUMNS) else edited_df

        # ã‚¹ãƒ†ãƒƒãƒ—5: CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.header("â‘¤ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        csv_bytes = to_csv_bytes(export_df)
        st.download_button(
            label=f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ{len(export_df)}ä»¶ï¼‰",
            data=csv_bytes,
            file_name="åˆ©ç”¨è€…æƒ…å ±.csv",
            mime="text/csv",
            use_container_width=True,
        )

        # ã‚¹ãƒ†ãƒƒãƒ—5': ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸åæ˜ ï¼ˆPoCï¼‰
        st.header("â‘¤' ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸åæ˜ ")

        spreadsheet_url = st.text_input(
            "Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL",
            value=st.session_state.get("spreadsheet_url", ""),
            placeholder="https://docs.google.com/spreadsheets/d/xxxxx/edit",
        )
        sheet_name = st.text_input(
            "ã‚·ãƒ¼ãƒˆå",
            value=st.session_state.get("sheet_name", "ã‚·ãƒ¼ãƒˆ1"),
        )
        st.session_state["spreadsheet_url"] = spreadsheet_url
        st.session_state["sheet_name"] = sheet_name

        if st.button(
            f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã™ã‚‹ï¼ˆ{len(export_df)}ä»¶ï¼‰",
            type="primary",
            use_container_width=True,
            disabled=not spreadsheet_url,
        ):
            try:
                with st.spinner("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ä¸­..."):
                    count = append_to_google_sheet(export_df, spreadsheet_url, sheet_name)
                st.success(f"{count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
            except FileNotFoundError as e:
                st.error(str(e))
            except Exception as e:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # å‡¦ç†å®Œäº†å¾Œã®ã‚¢ãƒ©ãƒ¼ãƒˆè§£é™¤
        remove_beforeunload_guard()


if __name__ == "__main__":
    main()
